{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvGeGKRUe3cv"
   },
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "\n",
    "Welcome to the wonderworld of CNNs™. If you followed the last lab session on pytorch, you should already be comfortable with Pytorch's autograd mechanism, and with the standard way to train a model. Today we will focus on convolutional neural nets, aka CNN, aka convnets. On the way, we will also get more familiar with the data loading mechanism from pytorch and the training loop. You have two goals:\n",
    "\n",
    "1. Overfit a small subset of MNIST. This is a fast and reliable way to test that a model is not garbage.\n",
    "2. reach the best possible test score on MNIST. Bon courage.\n",
    "\n",
    "# Réseaux de Neurones Convolutionnels (RNC)\n",
    "###### C'est un blague, personne n'utilise cet acronyme.\n",
    "\n",
    "Bienvenu dans le monde merveilleux des CNNs™. Si vous avez suivi la dernière démonstration sur pytorch, vous devriez déjà être à l'aise avec le mécanisme de différentiation automatique, ainsi qu'avec la méthode standard pour entraîner un modèle. Aujourd'hui,  on va s'intéresser à toutes les subtilités des réseaux de neurones convolutionnels. Au passage on va revoir le mécanisme de chargement des données et la boucle d'entraînement. Vous avez deux buts:\n",
    "\n",
    "1. mémoriser une petite fraction de MNIST avec un modèle. C'est une méthode simple et efficace pour vérifier qu'un modèle est décent.\n",
    "2. atteindre le meilleur score possible sur MNIST entier. Good luck.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XhfFTl1pe3cw"
   },
   "source": [
    "## Set up / Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "20cHv52zfXT8",
    "outputId": "fb89adfc-a570-423f-9b24-d635012e4f67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /anaconda3/lib/python3.6/site-packages (0.4.1)\r\n",
      "Requirement already satisfied: torchvision in /anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg (0.2.1)\r\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.6/site-packages (from torchvision) (1.14.3)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /anaconda3/lib/python3.6/site-packages (from torchvision) (5.1.0)\r\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.6/site-packages (from torchvision) (1.11.0)\r\n"
     ]
    }
   ],
   "source": [
    "# http://pytorch.org/\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "#!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "!pip3 install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "sJ07Jtthe3cy",
    "outputId": "857bedc5-f0e8-4f3c-ede1-6643d6ba13ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your version of Pytorch is 0.4.1. You should use a version >0.4.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(f\"Your version of Pytorch is {torch.__version__}. You should use a version >0.4.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "JizO0m1Ze3c6",
    "outputId": "ce407760-b936-41c6-a9e9-c4aac1652a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# If a GPU is available, use it\n",
    "# Pytorch uses an elegant way to keep the code device agnostic\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoIKi7ebe3c9"
   },
   "source": [
    "## Data / Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "MK9fKqhoe3c9",
    "outputId": "3ac92867-cd77-4743-ee6f-691132ffc7f9"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "train_data = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       # Standardize with mean and std computed on train set\n",
    "                       transforms.Normalize((0.1307,), (0.3081,)),\n",
    "                   ]))\n",
    "\n",
    "test_data = datasets.MNIST('../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lknHPBxSe3dA"
   },
   "outputs": [],
   "source": [
    "# size of the scratch training set\n",
    "n_scratch = 64\n",
    "\n",
    "# This parameter influences optimization\n",
    "batch_size = 64\n",
    "# This is just for evaluation, we want is as big as the GPU can support\n",
    "batch_size_eval = 512\n",
    "\n",
    "\n",
    "indices = list(range(len(train_data)))\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pr3AIrdce3dC"
   },
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "# This is the subset of MNIST we want to overfit\n",
    "scratch_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    # The sampler is an easy way to say that we're using the elements\n",
    "    # `indices[:n_scratch]` for this loader\n",
    "    sampler=SubsetRandomSampler(indices[:n_scratch]),\n",
    "    num_workers=1,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "# TODO: define a train, valid and test loader\n",
    "# size of the validation set\n",
    "n_valid = 15000\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(indices[n_valid:]),\n",
    "    #num_workers=1,\n",
    "    pin_memory=use_cuda\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size_eval,\n",
    "    sampler=SubsetRandomSampler(indices[:n_valid]),\n",
    "    #num_workers=1,\n",
    "    pin_memory=use_cuda,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size_eval,\n",
    "    #num_workers=1,\n",
    "    pin_memory=use_cuda,\n",
    ")\n",
    "\n",
    "# Extra Solution with `torch.utils.data.random_split`\n",
    "#n_total = len(train_data)\n",
    "#n_train = n_total - n_valid\n",
    "#train_data, valid_data = torch.utils.data.random_split(train_data, n_train, n_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "wFJcFGi6e3dG",
    "outputId": "75fe6017-dab2-4b59-aa49-70f789607c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the shape of one batch torch.Size([64, 1, 28, 28]). What is the meaning of each dimension?  batch size * channels * height * width\n",
      "target torch.Size([64]) {tensor(4), tensor(9), tensor(1), tensor(5), tensor(1), tensor(8), tensor(2), tensor(7), tensor(8), tensor(8), tensor(8), tensor(1), tensor(4), tensor(2), tensor(4), tensor(4), tensor(3), tensor(7), tensor(0), tensor(1), tensor(5), tensor(9), tensor(5), tensor(0), tensor(1), tensor(7), tensor(2), tensor(8), tensor(6), tensor(6), tensor(2), tensor(0), tensor(1), tensor(2), tensor(8), tensor(4), tensor(2), tensor(3), tensor(3), tensor(6), tensor(8), tensor(4), tensor(4), tensor(1), tensor(6), tensor(2), tensor(5), tensor(1), tensor(9), tensor(5), tensor(5), tensor(6), tensor(9), tensor(2), tensor(4), tensor(5), tensor(2), tensor(4), tensor(1), tensor(6), tensor(9), tensor(5), tensor(6), tensor(8)}\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize and understand the data\n",
    "for inputs, targets in scratch_loader:\n",
    "    print(f\"This is the shape of one batch {inputs.shape}. What is the meaning of each dimension?  batch size * channels * height * width\")\n",
    "    print(\"target\", targets.shape, set(targets))\n",
    "    img = inputs[0,0]\n",
    "    print(img.shape)\n",
    "    plt.imshow(img, cmap='Greys_r')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uU4lapoWe3dL"
   },
   "source": [
    "## Models / Modèles\n",
    "You can find below a basic CNN. You will have to define your own model a bit later. First let's try to train this one!\n",
    "\n",
    "Vous avez ci-dessous un CNN élémentaire. Vous devrez définir votre propre modèle un peu plus tard. Pour l'instant essayons déjà d'entraîner celui-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "30OitDs4e3dM"
   },
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    \"\"\"Affordable convolutions for the people.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.fc = nn.Linear(64*7*7, 10)\n",
    "\n",
    "    def forward(self, xin):\n",
    "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
    "        x = xin + F.relu(self.conv1(xin))\n",
    "        x = F.max_pool2d(x, 2) \n",
    "        # x is [bs, 32, 14, 14]\n",
    "        x = F.relu(self.bn(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2) \n",
    "        # x is [bs, 64, 7, 7]\n",
    "        x = x.view(-1, 64*7*7 ) # flatten\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrCD4Mjfe3dO"
   },
   "source": [
    "## Training / Entraînement\n",
    "\n",
    "You have to define general training and testing loops that can be applied to any pytorch module.\n",
    "\n",
    "Vous devez définir des boucles d'entraînement et de test générales qui s'appliquent à n'importe quel module pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Agd2qklAe3dP"
   },
   "outputs": [],
   "source": [
    "# Surrogate loss used for training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "# spot to save your learning curves, and potentially checkpoint your models\n",
    "savedir = 'results'\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6d6q1p3Ee3dS"
   },
   "outputs": [],
   "source": [
    "def train(model,train_loader, optimizer, epoch ):\n",
    "    \"\"\"Perform one epoch of training.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "        \n",
    "        # Let them code what's here\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(inputs), len(train_loader) *len(inputs) ,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ls48bZZGe3dU"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    \"\"\"Evaluate the model by doing one pass over a dataset\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, target in test_loader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            # let them code what's here\n",
    "            output = model(inputs)\n",
    "            test_size += len(inputs)\n",
    "            test_loss += test_loss_fn(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7092
    },
    "colab_type": "code",
    "id": "oDTy5iKHe3dY",
    "outputId": "76ef4801-38c2-40b5-e2b8-7f439dbba06b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/64 (0%)]\tLoss: 2.349960\n",
      "\n",
      "Test set: Average loss: 2.3370, Accuracy: 7/64 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/64 (0%)]\tLoss: 2.342157\n",
      "\n",
      "Test set: Average loss: 2.3259, Accuracy: 6/64 (9%)\n",
      "\n",
      "Train Epoch: 3 [0/64 (0%)]\tLoss: 2.327866\n",
      "\n",
      "Test set: Average loss: 2.3118, Accuracy: 6/64 (9%)\n",
      "\n",
      "Train Epoch: 4 [0/64 (0%)]\tLoss: 2.309736\n",
      "\n",
      "Test set: Average loss: 2.2982, Accuracy: 8/64 (12%)\n",
      "\n",
      "Train Epoch: 5 [0/64 (0%)]\tLoss: 2.291265\n",
      "\n",
      "Test set: Average loss: 2.2836, Accuracy: 11/64 (17%)\n",
      "\n",
      "Train Epoch: 6 [0/64 (0%)]\tLoss: 2.273449\n",
      "\n",
      "Test set: Average loss: 2.2722, Accuracy: 19/64 (30%)\n",
      "\n",
      "Train Epoch: 7 [0/64 (0%)]\tLoss: 2.259062\n",
      "\n",
      "Test set: Average loss: 2.2627, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 8 [0/64 (0%)]\tLoss: 2.247703\n",
      "\n",
      "Test set: Average loss: 2.2538, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 9 [0/64 (0%)]\tLoss: 2.237355\n",
      "\n",
      "Test set: Average loss: 2.2444, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 10 [0/64 (0%)]\tLoss: 2.227354\n",
      "\n",
      "Test set: Average loss: 2.2336, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 11 [0/64 (0%)]\tLoss: 2.216866\n",
      "\n",
      "Test set: Average loss: 2.2214, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 12 [0/64 (0%)]\tLoss: 2.205512\n",
      "\n",
      "Test set: Average loss: 2.2080, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 13 [0/64 (0%)]\tLoss: 2.193192\n",
      "\n",
      "Test set: Average loss: 2.1940, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 14 [0/64 (0%)]\tLoss: 2.180239\n",
      "\n",
      "Test set: Average loss: 2.1797, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 15 [0/64 (0%)]\tLoss: 2.167220\n",
      "\n",
      "Test set: Average loss: 2.1656, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 16 [0/64 (0%)]\tLoss: 2.154579\n",
      "\n",
      "Test set: Average loss: 2.1522, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 17 [0/64 (0%)]\tLoss: 2.142694\n",
      "\n",
      "Test set: Average loss: 2.1396, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 18 [0/64 (0%)]\tLoss: 2.131393\n",
      "\n",
      "Test set: Average loss: 2.1278, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 19 [0/64 (0%)]\tLoss: 2.120610\n",
      "\n",
      "Test set: Average loss: 2.1165, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 20 [0/64 (0%)]\tLoss: 2.110228\n",
      "\n",
      "Test set: Average loss: 2.1057, Accuracy: 19/64 (30%)\n",
      "\n",
      "Train Epoch: 21 [0/64 (0%)]\tLoss: 2.100107\n",
      "\n",
      "Test set: Average loss: 2.0951, Accuracy: 19/64 (30%)\n",
      "\n",
      "Train Epoch: 22 [0/64 (0%)]\tLoss: 2.090156\n",
      "\n",
      "Test set: Average loss: 2.0846, Accuracy: 19/64 (30%)\n",
      "\n",
      "Train Epoch: 23 [0/64 (0%)]\tLoss: 2.080101\n",
      "\n",
      "Test set: Average loss: 2.0743, Accuracy: 19/64 (30%)\n",
      "\n",
      "Train Epoch: 24 [0/64 (0%)]\tLoss: 2.070008\n",
      "\n",
      "Test set: Average loss: 2.0641, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 25 [0/64 (0%)]\tLoss: 2.060045\n",
      "\n",
      "Test set: Average loss: 2.0543, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 26 [0/64 (0%)]\tLoss: 2.050306\n",
      "\n",
      "Test set: Average loss: 2.0448, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 27 [0/64 (0%)]\tLoss: 2.041017\n",
      "\n",
      "Test set: Average loss: 2.0357, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 28 [0/64 (0%)]\tLoss: 2.032034\n",
      "\n",
      "Test set: Average loss: 2.0270, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 29 [0/64 (0%)]\tLoss: 2.023368\n",
      "\n",
      "Test set: Average loss: 2.0186, Accuracy: 20/64 (31%)\n",
      "\n",
      "Train Epoch: 30 [0/64 (0%)]\tLoss: 2.015015\n",
      "\n",
      "Test set: Average loss: 2.0104, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 31 [0/64 (0%)]\tLoss: 2.006958\n",
      "\n",
      "Test set: Average loss: 2.0026, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 32 [0/64 (0%)]\tLoss: 1.999146\n",
      "\n",
      "Test set: Average loss: 1.9949, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 33 [0/64 (0%)]\tLoss: 1.991572\n",
      "\n",
      "Test set: Average loss: 1.9874, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 34 [0/64 (0%)]\tLoss: 1.984136\n",
      "\n",
      "Test set: Average loss: 1.9801, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 35 [0/64 (0%)]\tLoss: 1.976860\n",
      "\n",
      "Test set: Average loss: 1.9731, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 36 [0/64 (0%)]\tLoss: 1.969758\n",
      "\n",
      "Test set: Average loss: 1.9662, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 37 [0/64 (0%)]\tLoss: 1.962869\n",
      "\n",
      "Test set: Average loss: 1.9596, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 38 [0/64 (0%)]\tLoss: 1.956265\n",
      "\n",
      "Test set: Average loss: 1.9532, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 39 [0/64 (0%)]\tLoss: 1.949875\n",
      "\n",
      "Test set: Average loss: 1.9471, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 40 [0/64 (0%)]\tLoss: 1.943678\n",
      "\n",
      "Test set: Average loss: 1.9411, Accuracy: 21/64 (33%)\n",
      "\n",
      "Train Epoch: 41 [0/64 (0%)]\tLoss: 1.937703\n",
      "\n",
      "Test set: Average loss: 1.9354, Accuracy: 22/64 (34%)\n",
      "\n",
      "Train Epoch: 42 [0/64 (0%)]\tLoss: 1.931903\n",
      "\n",
      "Test set: Average loss: 1.9298, Accuracy: 22/64 (34%)\n",
      "\n",
      "Train Epoch: 43 [0/64 (0%)]\tLoss: 1.926252\n",
      "\n",
      "Test set: Average loss: 1.9243, Accuracy: 22/64 (34%)\n",
      "\n",
      "Train Epoch: 44 [0/64 (0%)]\tLoss: 1.920798\n",
      "\n",
      "Test set: Average loss: 1.9191, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 45 [0/64 (0%)]\tLoss: 1.915530\n",
      "\n",
      "Test set: Average loss: 1.9140, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 46 [0/64 (0%)]\tLoss: 1.910396\n",
      "\n",
      "Test set: Average loss: 1.9090, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 47 [0/64 (0%)]\tLoss: 1.905401\n",
      "\n",
      "Test set: Average loss: 1.9043, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 48 [0/64 (0%)]\tLoss: 1.900536\n",
      "\n",
      "Test set: Average loss: 1.8996, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 49 [0/64 (0%)]\tLoss: 1.895810\n",
      "\n",
      "Test set: Average loss: 1.8952, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 50 [0/64 (0%)]\tLoss: 1.891256\n",
      "\n",
      "Test set: Average loss: 1.8908, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 51 [0/64 (0%)]\tLoss: 1.886877\n",
      "\n",
      "Test set: Average loss: 1.8866, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 52 [0/64 (0%)]\tLoss: 1.882618\n",
      "\n",
      "Test set: Average loss: 1.8825, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 53 [0/64 (0%)]\tLoss: 1.878466\n",
      "\n",
      "Test set: Average loss: 1.8785, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 54 [0/64 (0%)]\tLoss: 1.874409\n",
      "\n",
      "Test set: Average loss: 1.8746, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 55 [0/64 (0%)]\tLoss: 1.870440\n",
      "\n",
      "Test set: Average loss: 1.8708, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 56 [0/64 (0%)]\tLoss: 1.866536\n",
      "\n",
      "Test set: Average loss: 1.8671, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 57 [0/64 (0%)]\tLoss: 1.862374\n",
      "\n",
      "Test set: Average loss: 1.8629, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 58 [0/64 (0%)]\tLoss: 1.857705\n",
      "\n",
      "Test set: Average loss: 1.8580, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 59 [0/64 (0%)]\tLoss: 1.852782\n",
      "\n",
      "Test set: Average loss: 1.8523, Accuracy: 24/64 (38%)\n",
      "\n",
      "Train Epoch: 60 [0/64 (0%)]\tLoss: 1.846346\n",
      "\n",
      "Test set: Average loss: 1.8453, Accuracy: 24/64 (38%)\n",
      "\n",
      "Train Epoch: 61 [0/64 (0%)]\tLoss: 1.839705\n",
      "\n",
      "Test set: Average loss: 1.8390, Accuracy: 25/64 (39%)\n",
      "\n",
      "Train Epoch: 62 [0/64 (0%)]\tLoss: 1.833986\n",
      "\n",
      "Test set: Average loss: 1.8338, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 63 [0/64 (0%)]\tLoss: 1.828625\n",
      "\n",
      "Test set: Average loss: 1.8281, Accuracy: 24/64 (38%)\n",
      "\n",
      "Train Epoch: 64 [0/64 (0%)]\tLoss: 1.822793\n",
      "\n",
      "Test set: Average loss: 1.8217, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 65 [0/64 (0%)]\tLoss: 1.816868\n",
      "\n",
      "Test set: Average loss: 1.8161, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 66 [0/64 (0%)]\tLoss: 1.811536\n",
      "\n",
      "Test set: Average loss: 1.8108, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 67 [0/64 (0%)]\tLoss: 1.806547\n",
      "\n",
      "Test set: Average loss: 1.8058, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 68 [0/64 (0%)]\tLoss: 1.801638\n",
      "\n",
      "Test set: Average loss: 1.8007, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 69 [0/64 (0%)]\tLoss: 1.796533\n",
      "\n",
      "Test set: Average loss: 1.7954, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 70 [0/64 (0%)]\tLoss: 1.791101\n",
      "\n",
      "Test set: Average loss: 1.7895, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 71 [0/64 (0%)]\tLoss: 1.785294\n",
      "\n",
      "Test set: Average loss: 1.7831, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 72 [0/64 (0%)]\tLoss: 1.778847\n",
      "\n",
      "Test set: Average loss: 1.7763, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 73 [0/64 (0%)]\tLoss: 1.771743\n",
      "\n",
      "Test set: Average loss: 1.7694, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 74 [0/64 (0%)]\tLoss: 1.764503\n",
      "\n",
      "Test set: Average loss: 1.7626, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 75 [0/64 (0%)]\tLoss: 1.757405\n",
      "\n",
      "Test set: Average loss: 1.7554, Accuracy: 23/64 (36%)\n",
      "\n",
      "Train Epoch: 76 [0/64 (0%)]\tLoss: 1.750361\n",
      "\n",
      "Test set: Average loss: 1.7484, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 77 [0/64 (0%)]\tLoss: 1.743109\n",
      "\n",
      "Test set: Average loss: 1.7417, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 78 [0/64 (0%)]\tLoss: 1.736402\n",
      "\n",
      "Test set: Average loss: 1.7352, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 79 [0/64 (0%)]\tLoss: 1.729972\n",
      "\n",
      "Test set: Average loss: 1.7289, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 80 [0/64 (0%)]\tLoss: 1.723715\n",
      "\n",
      "Test set: Average loss: 1.7227, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 81 [0/64 (0%)]\tLoss: 1.717667\n",
      "\n",
      "Test set: Average loss: 1.7166, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 82 [0/64 (0%)]\tLoss: 1.711694\n",
      "\n",
      "Test set: Average loss: 1.7104, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 83 [0/64 (0%)]\tLoss: 1.705657\n",
      "\n",
      "Test set: Average loss: 1.7041, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 84 [0/64 (0%)]\tLoss: 1.699546\n",
      "\n",
      "Test set: Average loss: 1.6978, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 85 [0/64 (0%)]\tLoss: 1.693341\n",
      "\n",
      "Test set: Average loss: 1.6915, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 86 [0/64 (0%)]\tLoss: 1.687146\n",
      "\n",
      "Test set: Average loss: 1.6854, Accuracy: 27/64 (42%)\n",
      "\n",
      "Train Epoch: 87 [0/64 (0%)]\tLoss: 1.681029\n",
      "\n",
      "Test set: Average loss: 1.6794, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 88 [0/64 (0%)]\tLoss: 1.675101\n",
      "\n",
      "Test set: Average loss: 1.6737, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 89 [0/64 (0%)]\tLoss: 1.669399\n",
      "\n",
      "Test set: Average loss: 1.6681, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 90 [0/64 (0%)]\tLoss: 1.663842\n",
      "\n",
      "Test set: Average loss: 1.6628, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 91 [0/64 (0%)]\tLoss: 1.658398\n",
      "\n",
      "Test set: Average loss: 1.6574, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 92 [0/64 (0%)]\tLoss: 1.653067\n",
      "\n",
      "Test set: Average loss: 1.6522, Accuracy: 26/64 (41%)\n",
      "\n",
      "Train Epoch: 93 [0/64 (0%)]\tLoss: 1.647795\n",
      "\n",
      "Test set: Average loss: 1.6472, Accuracy: 28/64 (44%)\n",
      "\n",
      "Train Epoch: 94 [0/64 (0%)]\tLoss: 1.642755\n",
      "\n",
      "Test set: Average loss: 1.6425, Accuracy: 28/64 (44%)\n",
      "\n",
      "Train Epoch: 95 [0/64 (0%)]\tLoss: 1.637952\n",
      "\n",
      "Test set: Average loss: 1.6379, Accuracy: 28/64 (44%)\n",
      "\n",
      "Train Epoch: 96 [0/64 (0%)]\tLoss: 1.633312\n",
      "\n",
      "Test set: Average loss: 1.6334, Accuracy: 29/64 (45%)\n",
      "\n",
      "Train Epoch: 97 [0/64 (0%)]\tLoss: 1.628799\n",
      "\n",
      "Test set: Average loss: 1.6291, Accuracy: 29/64 (45%)\n",
      "\n",
      "Train Epoch: 98 [0/64 (0%)]\tLoss: 1.624390\n",
      "\n",
      "Test set: Average loss: 1.6247, Accuracy: 30/64 (47%)\n",
      "\n",
      "Train Epoch: 99 [0/64 (0%)]\tLoss: 1.620066\n",
      "\n",
      "Test set: Average loss: 1.6204, Accuracy: 31/64 (48%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BasicNet().to(device)\n",
    "\n",
    "lr = 0.0005\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "results = {'name':'basic', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
    "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    train(model, scratch_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, scratch_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    results['loss'].append(loss)\n",
    "    results['accuracy'].append(acc)\n",
    "    with open(savefile, 'wb') as fout:\n",
    "        pickle.dump(results, fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CvFTaNqe3dg"
   },
   "source": [
    "We have just applied our basic model on scratch_loader. A decent convnet with good parameters should be able to overfit this data easily.\n",
    "What happened ? What do you conclude ?\n",
    "\n",
    "On viens d'entraîner notre convnet de base sur scratch_loader. Un modèle décent devrait être capable de mémoriser ce dataset aisément. Que s'est-il passé? Qu'en concluez vous?\n",
    "\n",
    "\n",
    "## Build your model\n",
    "\n",
    "It's time to implement your own models to get the best clasification performance on MNIST.\n",
    "First, try to overfit scratch_loader. Once you succeed, replace it by the real loaders and classify these digits!\n",
    "You may consider the following ideas, ranked by relevance:\n",
    "\n",
    "* batch norm\n",
    "* more layers\n",
    "* skip connections\n",
    "* dropout (on the high level features)\n",
    "* data augmentation with `transforms.RandomRotation`or `transforms.RandomAffine` at the dataset creation time\n",
    "\n",
    "If you need some help to understand padding, stride etc..., [here](https://github.com/vdumoulin/conv_arithmetic) is very good resource from a Mila alumni.\n",
    "\n",
    "You can use the cell below to compare the learning curves of your models. Don't forget to change the `'name'`value in the `results`dictionary between each try.\n",
    "\n",
    "## Fabriquez votre modèle\n",
    "\n",
    "Vous devez maintenant implémenter votre propre modèle. Essayez d'abord de mémoriser scratch_loader. Ensuite essayez de bien classifier MNIST. Vous pouvez considérer les idées suivantes classées selon la préférence de l'auteur ce ces lignes:\n",
    "\n",
    "* normalisation de lot \n",
    "* réseau plus profond\n",
    "* connections sautées\n",
    "* abandon de neurones\n",
    "* augmentation de données avec les méthodes `transforms.RandomRotation`ou `transforms.RandomAffine`  lors de la création du jeux de données.\n",
    "\n",
    "Si vous ne connaissez pas ces notions, c'est normal. Regardez la version anglaise. \n",
    "Si vous avez voulez mieux comprendre le fonctionnement des couches convolutionnelles, regardez [ces merveilleux gifs](https://github.com/vdumoulin/conv_arithmetic) réalisés par un ancien du Mila.\n",
    "\n",
    "Vous pouvez utiliser la cellule ci-dessous pour comparer vos courbes d'apprentissages entre elles. N'oubliez pas de changer  la valeur `'name'` dans le dictionnaire `results`  entre chaque essais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "Etp_txmWe3dg",
    "outputId": "7e69babf-fcda-4b6b-bc96-a88d13041739"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f09e6fe2828>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAEICAYAAAAX5iNEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xd8lGW2wPHfTHrvEHoLHAhVOhZA\nUcS2Niy7NhTdte2y3uvuurp2Xbfo6q7u3UVXbGvFXlBRiqiIQITQD723kEYJpM79YyYYWjKBTN7M\n5Hw/n3yYed/3mTlJmMmZp5zH5fF4MMYYY4wxjcvtdADGGGOMMc2RJWHGGGOMMQ6wJMwYY4wxxgGW\nhBljjDHGOMCSMGOMMcYYB1gSZowxxhjjgHCnAzDGGCeJyJPAUMADTFDVeTXOrQc2AZW+Q1ep6pba\n2hhjjL+CJgnLy9tTr4JmKSmxFBaWBCqcgLLYnWGxO6O22DMyElyBfG4RGQF0VdVhItIDmAQMO+yy\nc1R1bz3bHKE+72Gh+vts6ix2Z4Rq7P68f4XscGR4eJjTIRw3i90ZFrszHI59FPA+gKouB1JEJDEA\nberFfp/OsNid0ZxjD5qeMGOMCYBMIKfG/Tzfsd01jv1bRDoC3wC/97PNEVJSYuv1hp2RkeD3tU2N\nxe4Mi90ZJxK7JWHGGPOjw4cP7gM+Awrw9n5d6kebo6rPcEtGRgJ5eXv8vr4psdidYbE7o7bY/UnO\nLAkzxjRnW/H2YlVrDWyrvqOqL1ffFpEpQO+62hhjjL9Cdk6YMcb4YSowFkBE+gNbVXWP736SiHwu\nIpG+a0cAS2prY4wx9WE9YcaYZktVZ4tIjojMBqqA20RkHFCsqu/5er/miMh+YAHwtqp6Dm/j2Ddg\njAlqloQZY5o1Vb3rsEO5Nc79Hfi7H22MMabebDjSGGOMMcYBIZeEeTwePvhmHSs3FjodijHGGGNC\nQEVlFe/OWsPWXfsa9HFDbjjyQFkln3y3nuk/bOG+6waSlhTtdEjGGNMkTZnyEWvXruH2239d77Zz\n5sxm27atXHzx2KOenzr1U95663VcLhcXXngx559/0SHnd+zYzsMP30dVVRVpaence+9DREZGHrVd\nRUUFjz76ANu3byMsLIzf//4+2rRpy+23/5wDBw4QHe19n7/99jvo3r1H/X8QxtQhd/UuPp69gZio\ncFqnxzXY44ZcEhYTFc5Pz+zGK58r//f+Eu66qj8R4SHX4WeMMY4aOvTkY57bv38/L7zwHM899zIR\nEeHceOO1DB9+OomJSQevef75iVxyyeWcccaZTJz4Tz755EPGjDnvqO2+/fZr4uMT+Ne/HmHu3DlM\nnPhPHnroMQDuvvs+OnfOCvj3a5q33NX5AHRvn9KgjxtySRjAyH6t2bxrHzNyNvPG9FVcM1qcDskY\nY5qkbdu2cOedv2Lnzh1cfvnPiIyM5O233yQszE3Hjl343e/uYfv27Tz88L243W4qKyu5776H+eGH\n+Qd70V599SVmzpyGy+Xmd7/7DUVFJfTo0ZP4+HgAevfuy6JFuZx66vCDz7tgQQ533vl7AE455TRe\nf/0V2rfvcNR28+fPZcyY8wAYOHAwjz32UCP/lExzVuXxsGjNLhLjIumQ2bCV/UMyCXO5XNw6ti8r\nNxYy44ctDOnRkm7tkp0Oyxhjjumt6auZt2InAGFhLior/d7v+5gGdW/B5WfU3ku0adNGJk16lX37\n9jJu3M+47rrxPPHE0yQkJHDbbTexZs1q5s2bw6BBQxg37kZUV7Br165D2s+cOY2JE19k69YtvP32\nq2Rn9yM5+cf33JSUVPLzdx3yvPv37ycyMrLG+Xzy8/OP2q6gIJ/kZG8PhNvtxuVyUV5eDsB//jOR\n4uIiOnToyIQJ/0tUlE1BMQ1r3bbd7C4p59Q+rXC7/Nogw28hO04XHRnOuDHdAfjvVKWyqsrhiIwx\npunp06cf4eHhJCUlExcXR1JSEr///f9y++0/Z8OGdRQXFzF48FA+++wTnn76ScrLy+jVq/fB9itX\nKtnZvXC73bRt245HH330iOfweGpPKI91vq7jl132U267bQL//OdzuN1u3nlnsr/ftjF+qx6K7JeV\n3uCPHZI9YdW6tEnitD6t+HrRNqbnbOGsQe2cDskYY47q8jOyDvZaNe5eeod+sn/ggXt4991PSEtL\n57e/9U7Y79w5ixdffJ25c+fw738/w3nn/eTg9WFhbqqqDk2W0tPTyc/PP3h/1648evbsfcg1MTGx\nlJYeICoqmry8naSnpx+zXXp6BgUF3uMVFRV4PB4iIiIYMeL0g9eecsppTJv2xQn+LIw50qLVuwgP\nc5HdsWHng0EI94RVu3RkF+Kiw3n/m7UU7y11OhxjjGlSli5dRGVlJYWFhezYsYOUlFTS0tLZsWM7\nK1Ysp6Kigi+//Jy1a1czfPhIbrrpVlSXH2wv0oPFi3OpqKigoCCf2267jZ49e7FixTL27NlDSUkJ\nixbl0rfvSYc878CBg5k5czoAX301nSFDTj5mu0GDhjJjxpcAfPvtLPr3H4jH42HChFvZs8ebrC5Y\nkEPnzl0a6admmouC3QfYuHMv3dunEB3Z8P1WAe0JE5G/AKf5nucxVX23xrmbgPFAJd4K1bep6olP\ngjhMYmwkl4zowiufK69PW8XNF/Zq6Kcwxpig1b59R+699y62bNnEnXfexfz5c7nxxmvJyurKz352\nDf/4x9/4/e/v48kn/0JMTCxut5tf//o3LFu2BIBWrVpz9tnncvvtP8fj8fDb395JVFQ0N998O//z\nP7fjcrm44YabiI+PZ9UqZdasmYwf/wvGj/8FjzxyHx988C6Zma0455zzCQ8PP2q7UaPOYv7877nl\nlvFERkZy993343K5+MlPLmbChFuIiYkhPT2DG274hcM/TRNqFq3x9sD2DcBQJICrrrH64yUipwO/\nUdVzRSQNWKCq7X3nYoGPgDGqWi4i04E/qOrsYz1eXt6eegVaszu/yuPhsVdyWLN1N7++rC99uqQd\n77fVKBp3KKJhWezOCNXYMzISGnYWrIPq8x4Wqr/Pps5id0ZTjv3vk3PJXZPPn28eRkZyzBHnT/T9\nK5DDkbOAy3y3i4A4EQkDUNUSVR3lS8BigSRge6ACcbtcXDemO2FuF698rpSWVQbqqYwxxhgTAkrL\nK1m2oZA26XFHTcAaQsCGI1W1Eqiu7z8emOI7dpCI3AVMAJ5S1bW1PV5KSizh4WH1iiEjI+GQ2xeP\nzOLt6av44octXH9Bz3o9VmOrGXuwsdidYbEbY0zDWb6hkPKKKvpkBW70LOCrI0XkQrxJ2OjDz6nq\nn0Tk78AUEflGVb891uMUFpbU63mP1kU46qTWzMzZxAez1jCgaxqt0hpu64GG1JS7ZutisTsjVGO3\n5MwY45RFq7217fp2Ccx8MAjw6kgRORu4BzhHVYtrHE8VkeEAqrof+BQ4JZCxAERFhHHFGV2prPLw\n+per6qxdY4wxxpjmx+PxkLsmn7jocLLaJNXd4DgFLAkTkSTgr8D5qlpw2OkI4EURiffdHwxooGKp\nqX+3dLI7prBkXcHBAmzGGGOMMdU27dxL4Z5S+nRJw+0O3PqgQA5HXgGkA2+JHNy7cTqwWFXfE5GH\ngBkiUoG3RMWHAYzlIJfLxc/O7Mb9k+by2pcrkfbJxESFdM1aY4wxxtTDwuqhyACVpqgWyIn5zwLP\n1nL+ReDFQD1/bVqnx3H24PZMmbOBVz5XbrogG1cD7wdljAkOIvIkMBTwABNUdd5RrnkMGKaqI0Vk\nJDAZWOo7vVhVf9lY8RpjAqN4Xxn/en8JJQcqyN+9H7fLRa9OqQF9zmbbBXTRaZ3QjYXMWbaD7h1S\nGN63tdMhGWMamYiMALqq6jAR6QFMAoYddk02MBwor3H4K1Ud23iRGmMCbe6yHazcVERURBhhbhfD\n+7YiNjoioM8Z8tsWHUt4mJtfXNiTuOhwXv1iJRu2B+fKMmPMCRkFvA+g3r14UkQk8bBrnsC7wMgY\nE8KqhyD/+POhPHPHcK4d0z3gz9lse8IA0pNiuOG8HjzzzmKempzLPdcMID1ABdmMMU1SJpBT436e\n79huABEZB3wFrD+sXbaIfAikAg+qap07R9e31mEwl+ew2J1hsR+/kgPlrNxURFbbJLp1rt88sBOJ\nvVknYQAndc3gylFdeX3aKp54K5e7r+5PQmyk02EZY5xxcHKoiKQC1wNnAm1qXLMKeBB4C+iMd4FR\nlqqW1fbA9al1GKp135o6i90ZTSH2+St2UlnlIbtDSr1iOdE6h812OLKmswa1Y8yQ9uwoKOGpybns\nL61wOiRjTOPYirfnq1prYJvv9hlABvA18B7QX0SeVNUtqvqmqnpUdQ3eLddqJmnGmCCT20irIQ9n\nSZjP2JFdOKV3Juu27eGpybm2v6QxzcNUYCyAiPQHtqrqHgBVfVtVs1V1KHAx8IOq3iEiV4nInb42\nmUBLYIsz4RtjTlRVlYdFa/NJioukQ2bjDotaEubjdrm4/pweDO7RglWbi3n63UWUV1Q5HZYxJoBU\ndTaQIyKzgX8At4nIOBG5uJZmHwIjRORr4APglrqGIo0xTdfabbvZU1JO36w03I1crqrZzwmrye12\nceP52ZSVV7Fw9S7+8/EyfnFhz0b/pRhjGo+q3nXYodyjXLMeGOm7vQe4IOCBGWMa3I6CEp6cnMuB\nGtOOynwdLoHcI/JYrCfsMOFhbm65qCfd2iYxb8VO22PSGGOMCRFL1xews3A/YWFu4mIiiIuJICUh\nip6dUukZ4MKsR2M9YUcRER7GL8f24U+v/sC0nM1kJMcwelA7p8MyxhhjzAnYXuBdpXz7Jb3p1Orw\nkoCNz3rCjiEuOoI7LutLYlwkb01fjW4sdDokY4wxxpyAHQX7AWiZ0jRqgloSVovUxGhuvagXLhf8\n6/0lFOw+4HRIxhhjjDlOOwpKSIyNCPh2RP6yJKwO3dolc8UZWewuKeffHyylsspWTBpjjDHBpqKy\nirzi/bRMjXU6lIMsCfPDqAFtGdyjBau3FDPluw1Oh2OMMcaYesor2o/HgyVhwcblcnHN2UJKQhQf\nfLOeddt2Ox2SMcYYE7Q8Hs8JfR2P6kn5mU0oCbPVkX6Ki47gxvN68Nc3FvLsR8t44PpBREX4vxmv\nMcYYY7z+9uZClq4/vgVvXVoncvc1A3DVs4ZndRLWMsWSsKDUo2Mqowe1Y+q8TXz47TouG5nldEjG\nGGNMUNlZWMLS9YUkx0eSmRpLREQ45eX+7dm8s2g/a7buZtPOvbRvWb8thnYc7AlrGisjwZKwert4\neGd+WJnH599vYkiPlvX+T2CMMcY0Z7mr8wG48NROjOjXhoyMBPLy9vjVds6y7Tz74TJy1+TX++/v\n9oL9uIAWTaQ8BdicsHqLigjjujHdqfJ4ePHTFbZa0hhjjKmH3DW7AOhzHNsE9erk3d9x0epd9W67\no6CEtKRoIsKbzlSigPaEichfgNN8z/OYqr5b49zpwGNAJaDAjaoaFBlNz06pDOuZyXdLtzMtZ4tV\n0zfGGGP8sL+0At1YRIeWCaQkRNW7fXxMBFltk1i1qYjd+8pIjIv0+3mL95XRy4GtiWoTsJ4wX5LV\nS1WHAWOApw675FlgrKqeAiT4rgkaV47KIi46nPe+XkvhnlKnwzHGGGOavKXrCqis8tA3K+24H6Nf\nVjoeYPHafL/b7Cj0TcpvQisjIbDDkbOAy3y3i4A4EanZBzhAVTf7bucBx/8bcUBCbCRjR3ahtKyS\nN6evcjocY4wxpsnL9Q0j9s2q/1BkteoEbmE9hiSbYnkKCOBwpKpWAvt8d8cDU3zHqs/vBhCRVsBo\n4N7aHi8lJZbweo7jZmQEdtL8JaOEOct2Mnf5Ti4Yvp9+3Vo02GMHOvZAstidYbEbY5qCkgMVlFVU\nHnHc44FFa/NJioukQ+bxv+YzU2NpkRzDknUFFOw+gNtdd6mKjTv2AtCyCa2MhEZYHSkiF+JNwkYf\n5VwL4CPgVlWttV+x0NeV6K/6rLY4EVeekcVDL83jmcm5PHTDYCLCT7xzsbFiDwSL3RmhGrslZ8YE\nl3XbdvPoyzlU1VJQ9bQ+rXDXs8ZXTS6Xiz5ZaXw5fzN3/t/serXNbEI1wiDwE/PPBu4Bxqhq8WHn\nEoFPgXtUdWog4wikDpkJnHFSW6b9sJkv5m/i3KEdnA7JGFMPIvIkMBTwABNUdd5RrnkMGKaqI/1t\nY0xztGHHHqo8HrLaJJGaeOTE+/AwN2OGtD/h5xk9sB37Sysor/B/PV9maixpSdEn/NwNKWBJmIgk\nAX8FzlTVgqNc8gTwpKp+FqgYGsvFwzsxd8UOPvp2PUOzW5Ka2LR+ycaYoxOREUBXVR0mIj2AScCw\nw67JBoYD5f62Maa5KvItVLvotE5kdwzcSsT05BjGn5cdsMdvLIGcmH8FkA68JSIzfV/3icjFIhIL\nXAvcWOPczwMYS0DFRkd4J+mXV/Lm9NVOh2OM8d8o4H0AVV0OpPh66Wt6Am+Pfn3aGNMsFe8rAyAp\nvv7lJ5qjQE7MfxZvGYpjCanf0Cm9WzFr4VbmrdjJiPUFAf0EYIxpMJlATo37eb5j1QuHxgFfAev9\nbXMs9V1cFMzz4Sx2ZzSF2PeXeYcHszqkEh/rXw0vaBqxH68Tid22LWogbpeLq0cLD700j/9OXcmD\nDTRJ3xjTqA7OFhaRVOB64EygjT9talOfxUWhutCiqbPYT9yOgn1EhLsp2XuA/fv8q6HZVGI/Hie6\nsMiyhAbUITOB009qw/aCEqbO2+h0OMaYum3F24tVrTWwzXf7DCAD+Bp4D+jvm5BfWxtjmrXivaUk\nxUXiOoHVj82JJWEN7JLhnUmMjeCj2evJLz7gdDjGmNpNBcYCiEh/YKuq7gFQ1bdVNVtVhwIXAz+o\n6h21tTHGCRWVVez2zcVyUlWVh+J9ZSQfx3ZEzZUlYQ0sNjqCy07Poqy8ile/WImnllopxhhnqeps\nIEdEZgP/AG4TkXEicnF92jROtMYc3buz1nLDI1MdT8T2lJTh8UCyn/s5GpsTFhAn98rk28XbWLh6\nFzmax8DuDVdJ3xjTsFT1rsMO5R7lmvXAyFraGOMIj8fDnKXbKS2rZNPOvfR0cIPqor22MrK+rCcs\nAFwuF9eO6U54mJtXv1jJvgPlTodkjDEmBG3csfdg8lO9P6JTivZ6J+Inx1tPmL8sCQuQzNRYLjil\nI8X7ypg8Y43T4RhjjAlBuWt+3MR6h8NJWHWNsGTrCfObJWEBdM6Q9rTNiGNW7laWrKt1a0xjjDGm\n3nJX5x/ch3F7PfdYbmjVPWFJ1hPmN0vCAig8zM3487IJc7t4YcoKSmxY0hhjTAMp3lvKum276dYu\nieSEKMd7wqqHRa0nzH+WhAVYh8wELji5I4V7Snn9y1VOh2OMMSZELFrjHWHpl5VOm4x4dhUfqNeG\n1g2t+OCcMEvC/GVJWCM4d1gHOmQm8O2S7cxfsdPpcIwxxgQpj8fDqs1FLF6bz3dLtwPQNyud1ulx\neDyQV7QfgMI9pY1esqJobynhYS7ioq3wgr8sCWsE4WFubjo/m8hwNy99toKC3VbE1RhjTP0tXLWL\nx/77A0++lcuKjUW0TI2lZWosbTLiAe/k/CqPhz++Mp/H31jQqLEV7S0jKS7KquXXgyVhjaR1ehxX\nntmVfQcqeO6jZVRVWRFXY4wx9ZOzMg+AMUPac+mIztz8k54AtPYlYdsLS1i3bTf5u0vZnLePbfn7\nGiWuKo+H3fvKrDxFPVmfYSMa0bc1S9cWkLMyj4+/W89PTunkdEjGGGOCRFWVh0Vr8kmKj2TsyC4H\nV0UCtMmIA7w9YftLKw4ez12dT6u0uIDHtreknMoqj80HqyfrCWtELpeL687pTmpiFB98s47lGwqd\nDskYY0yQWLttN3v3l9O3S9ohCRhAq/Q4XMD2gv3krs4nzO09v6hGHbFAsvIUx8eSsEYWHxPBzRf2\nwu1yMfHDpQf/4xpjjDG1yV3tTaj6dkk/4lxEeBhpSdFs2L6HTTv30qNDCp1aJbJyU3GjlEeyLYuO\njyVhDshqk8RlI7uwe18ZEz9YSmWVc0uKjTHGBIfc1bsID3OT3fHo+0NmpsZSWl4JeFdM9stKo8rj\nYcm6goDHVmxbFh0XS8IcctagdvTvloFuKuKdmWudDscYY0wTtqt4P5vz9tGjQwpRkWFHvaZlauzB\n2327pNE3y9tjVt2DFkhFtmXRcQnoxHwR+Qtwmu95HlPVd2uciwYmAj1VdWAg42iKXC4X48/rwZZd\n+/hs7kY6tU5kUPcWTodljDHmMHtKyli0Jh+Pg4vaV28pBqBvVtoxr8n0JWFtMuJIT47B4/GQkhDF\nojX5fLNoW0DjW7nRO8fZkrD6CVgSJiKnA71UdZiIpAELgHdrXPJXYCHQM1AxNHUxUeHcfklvHnlp\nPpM+WU7rtB9rvRhjjGkaXp+2ijlLdzgdBi7X0eeDVWvXwvv3o3/XDN/1Lvp1TWfGD1uYNGV5wONz\nu1ykJloSVh+B7AmbBcz13S4C4kQkTFUrfcfuBtKAqwIYQ5PXJj2OG87rwb/eX8LT7y7m3usGkuF0\nUMYYYwCoqKxi0ep8UhKiuPi0zo7GkpEcTVpS9DHPd2uXzG+u7EdW26SDxy4d3pms1klUNkJtyhYp\nMcRFRwT8eUJJwJIwX7JVXSVuPDClRgKGqu7x9ZA1e4O6t2DD0A5MmbOBiR8u5ZFbTnU6JGOaDRF5\nEhgKeIAJqjqvxrmb8L5/VQK5wG3ACGAysNR32WJV/WWjBm0azZotxZSUVjCkZ0tO7dPK6XDq1OOw\nSfux0REM65XpUDSmLgEv1ioiF+J9Ext9Io+TkhJLePjRJyMeS0ZGwok8ZaP6+aV92VG0n5wVO3ll\nyjLGnR+8o7TB9HM/nMXuDKdiF5ERQFfftIkewCRgmO9cLHAlcJqqlovI9OpzwFeqOtaRoE2jyl3t\n3SS7tmFAY45XnUmYiCSratHxPLiInA3cA4xR1eLjeYxqhYUl9bo+IyOBvLw9J/KUje76McLmHXt4\nZ8Zq0hIiGZodfJ9egvHnXs1id0ZtsTdCcjYKeB9AVZeLSIqIJKrqblUt8Z2vTsiSgO1A+0AHZZqO\nhat3ERnhpkeHZKdDMSHIn56w5SIyDXheVWf4+8AikoR38v2Zqhr4IiUhIDY6gl9e2odHX8nhxSkr\naJUaR4fM4O3dMCYIZAI5Ne7n+Y7trj4gIncBE4CnVHWtiLQHskXkQyAVeFBVv6jrierbm289m86o\nGfvWvL1sLyhhSM9MWrdq+klYqPzcg82JxO5PEtYeOBu4XkQeB94BXlDVuta7XgGkA2+JSPWx6Xjn\nT7wnIpOBdoCIyEzgWVV97Ti+h5DSOj2OO68awCOTvufpdxdx73WDSIqz4nfGNBLX4QdU9U8i8ndg\nioh8A6wCHgTeAjoDM0QkS1XLanvg+vTmh2rPZlN3eOwz5m0CoEf75Cb/PYXSzz2YnGhPfp1JmKqW\nAx8DH4tIN+B54A8i8i5wh6rmHaPds8CztTzuZXVG10wN7pnJJSM6885Xa3nm3UX89qf9iQi3urrG\nBMBWvD1f1VoD2wBEJBVvmZ1ZqrpfRD4FTlHVb4E3fdevEZHtQBtgXSPGbRrYjoISvlq8nb17Dxw8\nNmeZtyxF7862hswEhj9zwmKBscD1QCLwHHAuMAZ4G+9KIdPAzh3agc15+/h+2Q5e/nwFN5zbA5fr\niA/pxpgTMxVvr9ZEEekPbFXV6o+1EcCLItJHVfcCg4FXROQqoJWqPi4imUBLYIsTwZuG8/LnyvIN\nhUcc79ImkZQEq31lAsOf4ci1eHvCfqeqc2scnywiVwQmLONyubj+nO7sLCzh28XbaZMez5ghNh/Y\nmIakqrNFJEdEZgNVwG0iMg4o9k2beAjvcGMF3hIVHwLxwGu+ld+RwC11DUWapq3kQAUrNxXRITOB\nS4YfWgvM5uWaQPInCesGhAFdRWQQoKq6G+8NW6IdQJERYdx+SR8efmkek2esJjMtln5ZtkzamIak\nqncddii3xrkXgRcPO78HuCCwUZnGtHR9AZVVHk7p09qGHk2j8mei0Q3AauAp4Gm8cyBuCWhU5qCU\nhCh+eWkfIsLdTPxwKZt37nU6JGOMCSnVG1wP6hl8ZYFMcPMnCRsHdFbVk1V1KN6esV8ENCpziE6t\nErnx/GxKyyp56u1cCveUOh2SMcaEhKoqD4vW5JMcH0mXNkl1NzCmAfmThG2vWWhVVQuxVUCNbmD3\nFlw6ojMFu0v5+9u57C+tcDokY4wJemu37mbv/nL6dEm3xU+m0fk1MV9E3se7isgNnA7ki8gNAKo6\nKYDxmRrOHdqBXcUH+GrhVv71wRJ+dWkfwsOsdIUxxhyv3DXeoci+WTYXzDQ+f5KwGKAQGOS7vxvv\nRP3T8G54a0lYI3G5XFw9uhuFe0pZtCaflz5dwQ3nWekKY0zttuzax9e5W6nyeAh3uzlzYFtSE6Od\nDssvOwpLmLlgC5VVnoA8fo7mER7mJrtDat0XG9PA/CnWej0cLFzo8Q1HGoeEud3cfGFP/vr6Ar5d\nsp2k+CjGjuzidFjGOE5EXKoamL/UQW7yjNUsWpN/8P6B8kquPVtqadF0vP/1Or73FU0NlAGSQVSk\n/1tKGdNQ/CnWejLwCpAAuEQkH7haVecHOjhzdNGR4Uy4rC+PvZLDlDkbSIyLZPSgdk6HZYzTNojI\ny8AkVV3rdDBNRWl5Jcs3FNIqLZafX9CTx99YQO7qXXhGd2vyveiVVVUsXpNPamIUv7q0T8Cep1Va\nbMAe25ja+DMc+SfgQlVdAiAiJwF/B4YHMjBTu8TYSP7nin788b85vDFtFTFRYZzWp7XTYRnjpMF4\nd/eYJCLlwAvA2829kOryDYWUV1TRr2s6HTIT6N05jTnLdrBp517at2zahUhXby6mpLSCIdktm3ys\nxhwPf2Z1V1YnYACqugCwpXlNQEZyDHde0Y+46HBe/HQF81fsdDokYxyjqttV9RlVHQnc4vvaJiKP\niEhwTIAKgOoaWNWFnvv6/s2tMTzZVFXHaJPmTajyJwmrEpFLRCTR93U5UBnowIx/2mTEc8fl/YiM\nCGPih0tZ6HvDNaY5EpHhIjJ3U6FgAAAgAElEQVQJ+BT4FjgVKAImOxqYQzweD7mrdxEXHU6X1t4a\nWL06p+J2uVgUBO8Vuat3ERnupnv7FKdDMSYg/EnCbgZ+DmzAWx/sOt8x00R0bp3Ir8f2Iczt4v/e\nW8ySdU3/E64xDU1EVgP3AZ8B2ap6l6ouV9XHgWb5V3zjjr0U7S2jT5c03G7v/K+46Ai6tk1i7dbd\n7N7XdEdqdxaWsC2/hOyOqURG2KR5E5r8mRMWr6pjAh6JOSHSPoVfje3D399exNPvLOaXl/amVyfr\nwjfNyhjApaqrwDt/1Td9ArwldZqdH2tgHbrnbN+sdHRTEYvX5nNK71ZOhFYnG4o0zYE/SdgTwBmB\nDsScuOyOqfzy0t784+3F/ONtbyJmm9GaZmQc0BrvfrcAd4nIOl+PWMiXrlixoZCZC7cccmzlpiLC\n3C56dTq0BlbfrDTemrGaj2avZ/Ha4+85j44M47LTs4iLjjiu+GqzZstuAPp0Sa/jSmOClz9J2EYR\nmQnMAQ72XavqfYEKyhy/Xp3SmDC2D/94ZxFPv7OIWy/qTb+u9iZmmoXTVfWU6juqeoWIfONkQI1p\n8szVrNu254jjJ3VNJ/awJCkzNZaOmQms376HnYX7T+h5W6fH+1Ui51jx1aZ7+2RSEqKONzRjmjx/\nkrB1HLlXZMh/qgxmPTulehOxtxfxz/cWc9MF2Qzu0dLpsIwJtEgRiawuSSEi8UDdXTQhoHhvKeu2\n7aFbu2RuubDnIecS4iKPuN7lcnH3NQPYt7/8uJ9z7/5y7n1+Lrmrd9WZhNUWX20SYo+M3ZhQ4k8S\nVqyqT9U8ICIPBige00CyO6byP1f04+9v5zLxw6WUllVyWl+rI2ZC2r+B5SIyH+/WaoOABxyNqJFU\nV8Pv3zWdpHj/eo7Cw9x+X3s0SfFRdMxMYOWmIkoOVBAbfew/J7nHEZ8xzcExXzUicjreuWBX+7Ys\nqhYBXA/cX9eDi8hf8E6IDQceU9V3a5w7E/gj3nIXU1T14eP6DswxdWuXzG9+ehJPvLGQFz5dQUlp\nBWcPbu90WMYEhKo+LyJf4E2+PMAdePe6rZWIPAkM9bWZoKrzapy7CRiP930qF7hNVT21tXHCj5PY\nG3fqQb+sdNZv38PS9QUM6t7imNdV1yrra1MjjDlEbSUqVgDLfbcra3yVAFfW9cC+JK6Xqg7Du2rp\nqcMu+QdwKXAKMFpEsusXuvFHx8xE7rqqP8nxkbw5fTXvfLUGj8dGk03IigfygF1Ad7xzWY9JREYA\nXX3vU+Pxvi9Vn4vF+153mm+uWXdgWG1tnFBeUcXSdQW0TI2lZWrjbr9zsPBrLTXHyisqWba+kMzU\nWFqm2PZAxtR0zJ4wVd0GvCYis1V1/XE89ixgru92ERAnImGqWikinYECVd0EICJTgFHAsuN4HlOH\nNhnx3H31AJ54cyGffLeB4n1lXDdGCHP7UybOmOAgIn8HRgOZwGqgC/B4Hc1GAe8DqOpyEUkRkURV\n3a2qJb7z1QlZErAd7yrMo7YJwLdVJ91YSGl5JX27NP5K6PYt40mOj2TRmnyqqjwHa5EdGl+RNz4r\nNWHMEfyZEzZMRN4DUoGDrzBVrXVcS1UrgX2+u+PxDjlWV9rPxPtptdpOvG+Yx5SSEkt4eP0K9mVk\nBO9eYw0de0ZGAo9PGMGD//mObxZto7Siit9eM5DoSH/+C9T/uYKVxe6MBop9sKr2EJEZqnq6iAwA\nLq6jTSaQU+N+nu/YwYRKRO4CJgBPqepaEamzTWPKXe3MUCR4J/j36ZLOrNytrN22m6w2SUdcczA+\nKzVhzBH8+Qv8IHAj3or59SYiF+JNwkbXctmRH58OU1hYUq/nzchIIC+vfsuhm4pAxv4/l/fln+8t\nYd6yHfz2H18zYWwfEo+yeup42c/dGaEaez2Ts1Lfv1Ei4lLVHBGpqyfscEe8F6nqn3y9bFOOUfKi\nzvcvqP8HSX++d4/Hw+L1BcRFh3PySW0JD2v83u3h/dsyK3crL3y6grSkI7foXLulmLjocIY5FF99\n2YcZZzTX2P1Jwlap6qzjeXARORu4BxijqsU1Tm3F+8mxWhvfMRNg0ZHhTBjbhxc/XcHsJdt59JX5\n3HF5PzIbeS6JMQGgInIr3qkQX4iIAsl1tDn8vag1sA3AtyCpl6rOUtX9IvIp3jmsx2xTm/p8kPQ3\nqS4rr2RnQQnZHVMoLNhX5/WB0CY1hhYpMWzbtY9tu44ew+hB7RyLrz5C9cNMUxeqsfuTnPmThM0W\nkT8CM4GK6oOqOr22RiKSBPwVOFNVC2qeU9X1vs3AOwKbgfOBq/yIxTSA8DA348/rQXpSNB9+u55H\nX57P7Zf0RmyTXBPcbsa7R2QR3gn1LYHH6mgzFW9v/0QR6Q9sVdXqd9QI4EUR6aOqe4HBwCt4hx+P\n1aZRlVVUAQRkWoG/oiLC+NMvhh28H8x/UI1pbP68cs/0/TusxjEPUGsSBlwBpANviUj1senAYlV9\nD7gFeN13/E1VXelXxKZBuFwuLjqtM2mJ0bz8ufL4Gwu5/tzunNyrae4jZ4wfnlTVX/tuv+ZPA1Wd\nLSI5IjIbqAJuE5FxeOsjviciDwEzRKQCb4mKD30lKg5p0/Dfin/Kyr3TbCPDm/4wnzHmSHUmYap6\nOoBvjoXftQ1U9Vng2VrOz+LQxM444LS+rUlLiuaf7y3hPx8vZ1t+CRcP74zb5dc0F2OakkoROQOY\nzaFbrFXV1khV7zrsUG6Ncy8CL/rRxhHVPWGREZaEGROM6nzlikhfXwXq5b7794rIkIBHZhpNdsdU\n7rlmAC2SY/jkuw38893FHCirqLuhMU3LjcAXeGsZVvi+jn9fniDwY09Y/VaOG2OaBn+GI58BbgD+\n7rv/JvAC3gmqJkS0To/jD9cN5F/vL2HBql388ZUcbr+0Dy2SY5wOzRi/qOqR9RFCXHVPWIT1hBkT\nlPxJwspVdVH1vC5VXembH2FCTHxMBHdc3pc3pq1i+g9bePjFefziwp706mRFFk3T55u/dQRVva+x\nY2ks1T1hUdYTZkxQ8ufjU4WIdMI7GR8ROQc/6+KY4BMe5ubq0cK4c7pTWl7Jk2/m8tHs9VTZVkem\n6au5vVoYcDreKvchy3rCjAlu/vSE/S/wASAiUgysB64NZFDGecP7tqZNRhz/994S3pu1lrVbihl/\nfjbxMRFOh2bMUanqgzXvi0gY8I5D4TQKmxNmTHCr8+OTqi5W1T5AW6CdqvZV1dy62png16V1Evdf\nP4ieHVPIXZPPgy/MZfWW4robGtM0RABZTgcRSOXVqyOtRIUxQcnvCn+qmlf3VSbUJMZGcsfl/fh4\n9no++HYdf371By4Z3pmzh7S3MhamSRGRTfimTfikcpTyEqHkYE9YhPWEGROMnCuzbIKG2+3iJ6d2\nolu7ZCZ+tJTJM9ewZF0BN56fTUpClNPhGVPt1Bq3PcBuVS1yKpjGUGY9YcYENXvlGr9175DCgzcM\npm+XNJZvKOT+SXPJ0Z1Oh2VMtTjgZlXdoKobgSdFpKfTQQWS9YQZE9z8KdZ6johc7bv9qoisEpFL\nAh+aaYoSYyP51dg+XHVWN0rLK/nne0t4/pNl7C+1qiXGcf8EptS4/7zvWMg6uDrSesKMCUr+vHLv\nAz7zlaYIA04CfhXQqEyT5nK5GDWgLfePG0SHlgl8u3g79z3/PcvWF9Td2JjACVfVr6vvqOo3hHg5\nnbJybxIWZT1hxgQlf+aElajqLhE5D3hFVfeKSGWgAzNNX+v0OO65dgAfz17Px7M38PgbC1m6sYjz\nh7QnJsqmG5pGVywitwAz8X7AHAPscTSiACuv8L4VW0+YMcHJn1dutIj8Bu8b2jQR6UqIF0A0/gsP\nc3PRaZ35w3UDaJMex6ez13Pf89+zZG2+06GZ5ud6YADwFvA63vIU1zsaUYCVltsG3sYEM39euT8H\n2gDXq+oB4GzgroBGZYJOx8xE7hs3iCvO6kbR3jL+9lYuz320jD0lZU6HZpoJXxmdP6tqb19tw2dD\nvbROdU+YFWs1Jjj5k4StBJ5Q1a9FpA9QDMwObFgmGEWEu7l6TA/uvW4gHTIT+G7pdu557ntmL9mG\nx7Y9MgEmIo8Cv69x6C4R+ZNT8TSGgyUqrCfMmKDkzyv3JWCIiLQB3gV6E+IFEM2Jad8ygT9cO4Ar\nzsiirKKS/3y8nL++voCtu/Y5HZoJbSNV9YbqO6p6BYfWDgs5tm2RMcHNnySsjaq+DVwB/J+q/hZv\nJWpjjinM7ebswe155MYh9MtKZ8XGIu6fNJe3Zqy2chYmUCJFJLL6jojE4926KGSVVVQRHubC7Q7p\nRaDGhCx/lrBFiYgLuBgY7zsWH7iQTChJT4rhV2P7sGBlHq99uYrPvt/InKXbuWxkFkN7tsRlWx+Z\nhvNvYLmIzMdbTmcQ8FRdjUTkSWAo3ir7E1R1Xo1zpwOPAZWAAjcCw4HJwFLfZYtV9ZcN+H34ray8\nkgjrBTMmaPmThM3EOw/sM1VdKSK/xvtmZIzfTuqWQXanVKZ8t4FPv9/Icx8vY/qCzfx0VDc6t050\nOjwTAlT1eRFZBaTjTag+xDtH7MljtRGREUBXVR0mIj2AScCwGpc8C5yuqptFZDLeVeIlwFeqOjZA\n34rfyiqqbD6YMUGszlevqt4FtFfVy32H3sf7abBOItJLRNaIyO1HOXehiMwTkW+Odt6EnqiIMC4e\n3plHbxrCQMlgzZbdPPLyfJ79cCm7ivc7HZ4JciLyFDARb6X8u/H2gr1SR7NReN/TUNXlQIqI1PxU\nMEBVN/tu5wFpDRr0CSorr7R9I40JYnX2hIlIK+ARERmE99PlHOAPeN+QamsXBzwNTDvKOTfwDNAf\nyAc+FZH3a7zZmRCWkRzDrRf3RjcW8sa01cxZtoP5mseoAW04b1hH4mNCehqPCZwhqtpDRGao6uki\nMgDvNIraZAI5Ne7n+Y7tBlDV3XDwfXA0cC/exUnZIvIh3vmxD6rqF3UFl5ISS3g9hg4zMhLqvKai\n0kNSfIRf1zamphZPfVjszmiusfszHPks8BnwN7xbgJyJ95PmT+poVwqcC/zuKOfSgaLqGj4iMs33\nuC/6FbUJCdI+hXvHDeS7Jdt5/+u1fD53E1/nbuOcoe05c0A7oiJtroupl1Lfv1Ei4lLVHBF5vJ6P\nccQkRRFpAXwE3Kqq+b4hzwfxFoXtDMwQkSxVrbUoXmFhid9BZGQkkJdXd7H/A2WVuF34dW1j8Tf2\npshid0aoxu5PcuZPEharqjU3wV0iInUlYKhqBVAhIkc7nQck+KrvrwdOxzv37Jjq+ykSmm9m7bT6\nxn7RGYmce1oXPvl2HZOnreSdr9Yy7YctXHZGV84e1rFR98VrTj/3pqSBYlcRuRWYBXwhIgok19Fm\nK96er2qtgW3Vd3xDk58C96jqVABV3QK86btkjYhsx1vQel1DfBP+qvJ4qKissvIUxgQxf5KwOBFp\nparbAESkLRB9Ik+qqh4RuQ7vJNhivG9etS6Tq8+nSAjdzLqpO5HYT+3ZkgFZaXw+dyOfz9vEcx8s\n4a1pKzlvaAdG9Gsd8FVgzfXn7rQT/SRZw81AClAEXAm0xLuysTZT8fZqTRSR/sBWVa0ZzBPAk6r6\nWfUBEbkKaKWqj4tIpu95ttQn0IZQ7tuyKMIm5hsTtPxJwh4Gcnyf9lxABj+WqjhuqvoVcBqAiDyG\nt0fMNHMxUeFcdFpnRg1oy2dzNzItZzOvfbmKT+Zs4Jwh3mSsMXvGTPBQVQ9Q4Lv7mp9tZotIjojM\nBqqA20RkHN4Ph58D1wJdRaR6MdJrePelfE1ELgQigVvqGooMhDLflkVR1hNmTNDyJwmbAnQBuuGd\nmL/St4fkCRGRT4HrgH3ABXg/cRoDQEJsJJeNzOLsQe35fN5Gpuds4Y1pq/jku/WMHtSOM/q3JSbK\nn/++xtTOtwK8ptwat6OO0eyCAIXjtzLrCTMm6PnzV2y6qp7OoW9MdfKtTHoC6AiUi8hYvHV71qnq\ne8BzeIcCPMBjqrqrPo9vmofEOG8yNmZwe76Yv5lpOZt556u1TJmzkdNPasNZA9uSFH+sv5PGhK4y\n27zbmKDnTxK2UEQewrtp98Eud1WdXlsjVc0BRtZy/l28e1EaU6eE2EguGd6ZMYPbM2PBZr6Yt4kp\nczYwdd4mTu6VydmD29EqLc7pMI1pNNU9YVYnzJjg5U8S1s/372k1jnmAWpMwYwIhNjqc84Z15KyB\n7fh28TY+n7uJWblb+Tp3K32z0jl7cDu6tUu27ZBMyCuv8CVhNkfSmKBVZxLmK3qYWKNoYaaqbg98\naMYcW2REGKf3b8uIfm34YWUen83dyMLVu1i4ehcdWiZw1qC2DO7RkvAw6yUwoan04HCk/R83JljV\n+er11d15ucah12ybIdNUuN0uBnZvwR+uHcjdVw9gQLcMNu7cw38+Xs5v/zWbqfM2UVpe6XSYxjS4\n6hIV1hNmTPDyZzjyGg4dihyNtxjiMwGJyJjjlNU2iay2vdlVtJ8vczbz1cKtB1dUXnRqJ4b3a02Y\n23oNTGionpgfYT1hxgQtf169Yb7q99U81FFY1RgnpSfHcOWorvz11pO54OSOlFdU8crUlTz4wjxW\nbipyOjxjGsTBiflWosKYoOVPT9iHvkKGX+NN2kYB7wQ0KmMaQHxMBBcP78wZA9ryzldr+HbRNv78\n6g+MGtCWS0d0sb0pTVCzEhXGBL86P0Kp6iPAb4GdePdUu1VVHw10YMY0lKS4SG44twd3XzOAzLRY\nvszZzAMvzGVL3l6nQzPmuFlPmDHBz6+S46r6DfBNgGMxJqC6tEni/nGDeHfWWqbO28Qjr+Tw8wuy\nOalrhtOhGVNv1hNmTPCzj1CmWYmMCOPKUV25+cKeeKo8PPPOYt6dtZbKqiqnQzOmXn6sE2Zv48YE\nK3v1mmZpcI+W/P7qAaQlRfPx7PX89bUF7Cra73RYxvituvSK9YQZE7wsCTPNVofMBB64fhADJYOV\nm4u59S/TmTpvk/WKmaBgPWHGBD979ZpmLTY6glsu6sW4c7oT5nbxxrRVPPjCfDZs3+N0aMbUqsx6\nwowJepaEmWbP5XIxvG9r/n3XKE7t04rNeXt5+KX5vDdrLRWV1itmmqbq1ZER1hNmTNCyV68xPknx\nUdxwbg/+94p+JCdE8tHs9fzp1R/YVWxzxUzTU+YbjoyynjBjgpZfJSqMaU56dkrl4fFD+O9U5bul\nO3hg0jxuOK8H/btZKYtQJCJPAkPx7gYyQVXn1Th3OvAYUAkocKOqVtXWprFUD0daT5gxwctevcYc\nRUxUODeen824c7pTXlnFM+8u5tmPlrJ3f7nToZkGJCIjgK6qOgwYD/zjsEueBcaq6ilAAjDGjzaN\noqyiivAwN26X7SJnTLCyJMyYY6ieK3bfuEF0apXInKU7+MNzc5izdDsej8fp8EzDGAW8D6Cqy4EU\nEUmscX6Aqm723c4D0vxo0yjKKiqJtM27jQlqNhxpTB3apMdx9zX9mTpvEx98vY5nP1rG14u2cfXo\nbrRKi3M6PHNiMoGcGvfzfMd2A6jqbgARaQWMBu7FOzx5zDbHkpISS3g95m9lZCTUer6qCqKjwuu8\nzglNMSZ/WezOaK6xBzQJE5FewAfAk6r6zGHnbgOuxjvXYr6q/jqQsRhzIsLcbs4Z0oGB0oJXv1jJ\nojX53Pf8XM4a2I4LTulITJR9ngkRR4ztiUgL4CO8++bmi0idbY6msLDE7yAyMhLIy6u9TEpJaTlR\n4WF1XtfY/Im9qbLYnRGqsfuTnAWsL1tE4oCngWlHOZcI/AY4TVVPBbJFZGigYjGmoWQkxzBhbB9u\nv6Q3KQlRfDZ3I3c/N4fvbIgyWG3F24tVrTWwrfqO773qU+APqjrVnzaNpby8ygq1GhPkAvkKLgXO\nxfuGdbgy31e8iIQDsUBBAGMxpsG4XC76d8vgkRuHcNGpnSg5UMFzHy3jz68tYMuufU6HZ+pnKjAW\nQET6A1tVtebH2ifw9uR/Vo82jaKsopIIK09hTFAL2BiKqlYAFUfpukdVD4jIg8BaYD/whqqurO3x\n6jufAprvGLPTmlPs4y9O5vwRWfzng8XMWbKdB1+Yy8Ujs7jiLCEqonH/QDann3tDUdXZIpIjIrOB\nKuA2ERkHFAOfA9cCXUXkRl+T11T12cPbNHbcVVUeKio9RFlPmDFBzZGJLL4u/ruBbngns04Xkb6q\nmnusNvWZTwGhO8bc1DXH2N3Az8/PZrC04NUvlMnTVvHNwi3ceH42nVo1zqK5UP25N0Zypqp3HXao\n5vtQlJ9tGlVZha9GmPWEGRPUnPoY1QNYq6q7VLUM+BoY4FAsxjSIfl3TefjGIZw5oC3b8kt49OUc\n3v96LVVVNlfMNKwy27zbmJDg1Ct4PdBDRGJ89wcCqxyKxZgGEx0Zzs/O6sZvruxHSkIkH367nsff\nWEDR3lKnQzMh5MfNuy0JMyaYBWw4UkQG4J3U2hEoF5GxwIfAOlV9T0T+CswQkQpgtqp+HahYjGls\nPTqm8sANg5n0yXIWrNrFA5PmcstFvZD2KU6HZkJA+cGeMBuONCaYBXJifg4wspbzE4GJgXp+Y5wW\nFx3B7Zf05ov5m5k8YzWPv7GQK87IYtSAtrhsqxlzAsrKvUlYhPWEGRPU7BVsTAC5XC5GD2rHnVf2\nIy46nNe+XMWkKcsp902sNuZ4VE/Mb+wVuMaYhmVJmDGNQNqncN+4QXTMTODbxdv582sLKNxj88TM\n8bGeMGNCg72CjWkkqYnR3HVVf4b1bMnarbt56KV5rN5c7HRYJghV94RFWokKY4KaJWHGNKLIiDBu\nPD+bK87IYve+Mv782g/MXLjF6bBMkKnuCbMSFcYEN3sFG9PIXC4XZw9uz/9e0Y+YqHBe/kx5ZapS\nWVXldGgmSBwoqwBsTpgxwc6SMGMckt0xlfvGDaRtRjwzftjCU5MXUXKg3OmwTBAo3lsGQFJ8pMOR\nGGNOhCVhxjgoPSmG31/dn75d0li6roBHX8lhR0H9tugyzU/RPm8Slhx/1F2VjDFBwpIwYxwWExXO\nLy/tw9mD27Etv4SHX5rP0nUFTodlmrBi3w4MloQZE9wsCTOmCXC7XVxxRlfGn9eDsopK/vbWQqbM\n2YDHY/tOmiMV7S0lPMxFXHTA6m0bYxqBJWHGNCGn9G7F737Wn+T4KN6euYZn3l1MyYEKp8MyTUzR\n3jKS4qJs5wVjgpwlYcY0MV3aJHH/uEF0b5/MglW7eOileWzcscfpsEwTUeXxsHtfGck2Kd+YoGdJ\nmDFNUGJcJP97ZT/OHdqBnYX7eeTlHL5auMWGJw17S8qprPLYfDBjQoBNKDCmiQpzuxk7sgtZbZN4\n/uNlvPSZohuLuOZsISbKXroNRUSeBIYCHmCCqs6rcS4amAj0VNWBvmMjgcnAUt9li1X1l40Vb5Fv\nUr6VpzAm+Nk7uTFNXL+sdO6/fhATP1jKnGU7WLdtNzdf2IsOmQlOhxb0RGQE0FVVh4lID2ASMKzG\nJX8FFgI9D2v6laqObaQwD1F0sEaY9YQZE+xsONKYIJCeFMPvrurPmMHt2VG4n0dens9n32+kyoYn\nT9Qo4H0AVV0OpIhIYo3zdwPvORHYsfxYnsJ6wowJdtYTZkyQCA9zc/kZWWR3TOE/nyznrRmrWbq+\ngPHn9SAjw3rFjlMmkFPjfp7v2G4AVd0jImlHaZctIh8CqcCDqvpFXU+UkhJLeD023D7W77Tcl3d3\naJPcZH/vTTUuf1jszmiusVsSZkyQ6dU5jYduGMykKctZtCaf+56fyx0/60+njDinQwsF/tR8WAU8\nCLwFdAZmiEiWqpbV1qiw0P+dEDIyEsjLO/qK2C07vcddlVXHvMZJtcXe1FnszgjV2P1Jzmw40pgg\nlBgXyYSxffjZmV05UFbJw89/z6tfrKS8otLp0ILNVrw9X9VaA9tqa6CqW1T1TVX1qOoaYDvQJoAx\nHsL2jTQmdFgSZkyQcrlcnDmwHfddN5B2LROYlrOZh1/KYeuufU6HFkymAmMBRKQ/sFVVa/1ILiJX\nicidvtuZQEtgS6ADrVa0t5Qwt4v4mIjGekpjTIAEdDhSRHoBHwBPquozNY63AV6tcWln4C5VfS2Q\n8RgTitq2iOdvvx7OP99cwMyFW3nopXlcdVY3Tu3dyiqq10FVZ4tIjojMBqqA20RkHFCsqu+JyGSg\nHSAiMhN4FvgQeE1ELgQigVvqGopsSMV7S0mKj8Rtv1tjgl7AkjARiQOeBqYdfk5VtwAjfdeFAzPx\nvrEZY45DdGQ4147pTnbHVF74dAUvTFnB8vWFVlPMD6p612GHcmucu+wYzS4IXETH5vF4KNpbRvuW\nwTuJ2Rjzo0AOR5YC5+Kdc1GbccA7qro3gLEY0ywM7N6CB68fRJfWicxZtoMHX5zHhu3BOeHVHGnv\n/upq+TYfzJhQELCPyKpaAVSISF2X3giMruui+i7vhua75NVpFrszqmPPyEjg8V+n899Pl/POjNU8\n+koON1zQk/NP7dRkhyeD+efemKon5duWRcaEBkfHKURkGLBCVXfXdW19lndD6C55beosdmccLfbz\nhrSnfUYc//l4Gc++v5h5S7dx/bk9mtyE7hNd4t2c2JZFxoQWp1dHng986XAMxoSs3p3TeOD6wXRv\nn8yCVbu4f9JcdGOh02GZ41RkPWHGhBSnk7BB1JgEa4xpeCkJUdx55UlcPLwzxXvL+MvrC3h31hoq\nKqucDs3UU/E+27LImFASyNWRA4AngI5AuYiMxbsCcp2qVu/F1grYGagYjDFebreLC07uSI/2KTz7\n0VI+nr2BJWsLuOmCbFqlWaX9YFG0x3rCjAklgZyYn4OvDEUt1/QO1PMbY46U1TaJB28YzGtfrOTb\nJdt54IV5XDq8M2cObIfb3TQn7ZsfFe2rnhNmSZgxocDp4UhjTCOLiQpn/PnZ3HpRL6Iiwnhj+mr+\n/NoPbC+o3+IX0/h2FXFPSeEAAAyCSURBVB8gPMxNQmzTWlxhjDk+loQZ00wN7N6CR24cwkDJYNXm\nYu6fNJdP52ygssrmijVFHo+HHQUltEyNsWr5xoQIS8KMacYS4yK59eLe3HpRL2Kiwpk8cw0Pvzif\nddvqrBpjGtnufWUcKKskMyXW6VCMMQ3EkjBjzMFesVN7t2Ljzr088vJ8XvtyJftLK5wOzfhUDxe3\nTLUkzJhQYUmYMQaA+JgIbjivB7/56Um0SI7hy/mb+cN/vidH85wOzQA7Cvf/f3t3HiRFecZx/Duy\nuhyiLIisXILXE4GoQFjkEhBKVDwiYtR4hISI8Ug0GmMSlagxMcEi4EFFjFHLK1qJhWI8AyUqKtYC\nUTzgSZBDDjlWruUU2M0f3ZsMCwu7y/Z29/j7VFE108fwTM/OM0+//fb7AtCqeaOYIxGRuqIiTER2\ncfyRBdw5sohz+nRgw6avmDDpI+77+xxK1m2JO7SvtYqWsEK1hInkjFinLRKRZDowrwHf7ncUPTu1\n4onXnA/ml/DpojUM7d2B04vacWAN53GV/bdSlyNFco5awkSkSke0aMJNF3flirM60fCgBkx6awG3\n/Dm4RFleXh53eDnplRmLGf/M7N2Wr1izmcb5eTRN2NyfIlJ7KsJEZK8ymQy9uhTyu1G9OK1HO9aW\nbmPCpI8Y8/S/WLRCd1HWtYUrSplavGSXcdvKyspZtXYLrZo3JqPhKURyhoowEamWxg3zuGjQsdw5\nsoiTjjkMX7KOOx+byUOTP2GV+ovVmW8e1RyAD+eX/G9ZyYat7Cwrp1Cd8kVyivqEiUiNHNGiCT8Z\nfgJzF63h2TfmM+PTlRTPW8WAk9pwVu8jUzeljpmNA04GyoHr3L04a11DYCLQ2d2/VZ199tcJRx8G\nBEXYkKL2AKz4Uv3BRHKRWsJEpFaO79Cc0SN6cOU5nWl+SD5TZy/l5onv8dybn7Fxy/a4w6sWM+sP\nHOvuvYCRwH2VNrkH+KCG++yXQ5scxHHtm/GfpevZvDU4jit1Z6RITlIRJiK1dkAmQ89OrfjtFSdz\n2RCjUX4eL723mJsffJcXpi9k89bED/Y6CHgewN3nAgVmdkjW+l8Bk2q4z37r0amQnWXlfLxwDQAr\n1oYtYRotXySn6HKkiOy3vAYHMLBrG3p3KeSN2ct4ecZiXpi+kCkzl3BaUXsGd29Lo/xEpptCYFbW\n89Xhsg0A7l5qZi1qsk9VCgoak1fNoT2KOhXy1KvzmLd0PUNPOYa1pV8B0Pm4w5N6HHfRsmXTuEOo\nNcUej69r7Mn/NotIauQf2IDTe7ZnQNfWTJ21lFff/5xJby3gn8VLGFLUjlO7JbYYq1CbWw+rtc/a\ntZv3vVGoY+tDKGiaT/EnK1i5cgNLVm6goGk+GzdsYWMtAqxPLVs2ZfXq0rjDqBXFHo9cjb06xZku\nR4pInWt4UB5De3VgzFW9Oa9fR8rKynnuzQXc+vD7SbtEuZygFatCa+CLCPapkUwmw4lHt2DT1h1c\n/cc3+XLDNloV6M5IkVyT6FNSEUm3Rvl5nN2nI4O6t2PKzCUsK9lEXoNEjXP1OnAHMNHMugHL3X1f\np+S12afGBnZry7KSTWzfUUYmk2Fgt7Z1/V+ISMxUhIlI5Bo3zOOcvh3jDmM37v6umc0ys3eBMuAa\nMxsBrHf3SWb2N6AdYGY2DXjI3Z+uvE8UsbU7/GB+eWn3KF5aRBIi0iLMzLoALwDj3P2BSuvaAX8F\nDgJmu/uPooxFRGRP3P0XlRZ9mLXugmruIyJSY5H1CTOzJsD9wNQqNhkLjHX3ImCnmbWPKhYRERGR\npImyJWwbcCZwc+UVZnYA0A+4GMDdI2nOFxEREUmqyFrC3H2Hu1c1oVxLoBQYZ2bTzezuqOIQERER\nSaK4OuZngDbAvcAi4CUzG+ruL1W1Q00GOqzwdR38LW6KPR6KXUQkXeIqwkqAxe7+GYCZTQU6A1UW\nYTUZ6BByd/C3pFPs8cjV2FWciUgui2WwVnffASwws2PDRd0BjyMWERERkThE1hJmZt0J7oDsAGw3\ns+HAZGChu08CrgceCzvpfwS8GFUsIiIiIkmTKS8vjzsGERERka8dzR0pIiIiEgMVYSIiIiIxUBEm\nIiIiEgMVYSIiIiIxUBEmIiIiEgMVYSIiIiIxUBEmIiIiEoO4pi2KjJmNA04GyoHr3L045pD2yczG\nAP0IPo+7gWLgCaAB8AVwmbtviy/CqplZI+Bj4DfAVFISN4CZXQL8HNgBjAbmkIL4zexg4HGgAMgH\n7gBWAH8i+Luf4+5XxRfh7sysC/ACMM7dHzCzduzhWIefyfVAGfCQu/8ltqBjkrYclub8BenNYcpf\n9SuqHJZTLWFm1h841t17ASOB+2IOaZ/MbCDQJYz5dGA8cCcwwd37AfOBH8QY4r7cCqwJH6cmbjNr\nAfwa6AucBZxLeuIfAbi7DwSGA/cS/N1c5+59gEPN7IwY49uFmTUB7if4gauw27EOtxsNDAYGAD81\ns+b1HG6s0pbDciB/QQpzmPJX/Yoyh+VUEQYMAp4HcPe5QIGZHRJvSPv0FnBB+Hgd0ITgw5scLnuR\n4ANNHDP7BtCJ/0+8PoAUxB0aDExx91J3/8LdR5Ge+EuAFuHjAoIfkI5ZLSZJi30bcCawPGvZAHY/\n1j2BYndf7+5bgHeAPvUYZxKkLYelNn9BqnOY8lf9iiyH5VoRVgisznq+OlyWWO6+0903hU9HAi8D\nTbKakVcBR8QS3L6NBW7Iep6WuCGY07SxmU02s7fNbBApid/dnwHam9l8gh/BnwFrszZJVOzuviNM\nSNn2dKwrf38T9T7qSapyWMrzF6Q3h3VA+aveRJnDcq0IqywTdwDVZWbnEiSxayutSuR7MLPLgffc\nfWEVmyQy7iwZgrOxYQTN44+ya8yJjd/MLgU+d/djgFOBJyttktjYq1BVvGl7H1FIxTFIW/6C1Ocw\n5a9kqXUOy7UibDm7njW2Jugwl2hmNgS4BTjD3dcDG8POogBt2LUJNCmGAuea2Qzgh8BtpCPuCiuB\nd8MznM+AUqA0JfH3AV4DcPcPgUbAYVnrkxx7hT39rVT+/qbhfdS11OWwlOYvSHcOU/6KX53ksFwr\nwl4n6OiHmXUDlrt7abwh7Z2ZHQrcA5zl7hWdQ6cA54ePzwdejSO2vXH3C929h7ufDDxMcGdR4uPO\n8jpwqpkdEHZyPZj0xD+foO8BZnYkQQKea2Z9w/XDSG7sFfZ0rN8HephZs/AOqj7A2zHFF5dU5bC0\n5i9IfQ5T/opfneSwTHl5eaRR1jcz+z1wCsHtodeElXZimdko4Hbg31mLv0eQFBoCi4Hvu/v2+o+u\neszsdmARwdnN46Qn7isJLqEA3EVwa33i4w+/3I8ArQiGBbiN4BbviQQnVu+7+w1Vv0L9MrPuBH1v\nOgDbgWXAJcBjVDrWZjYcuIngVvX73f2pOGKOU5pyWC7kL0hnDlP+qj9R5rCcK8JERERE0iDXLkeK\niIiIpIKKMBEREZEYqAgTERERiYGKMBEREZEYqAgTERERiYGKMEktMxthZpVHWxYRSTzlLwEVYSIi\nIiKx0DhhEjkz+zHwHYKB+eYBY4B/AK8AJ4abXeTuy8xsKDAa2Bz+GxUu7wmMB74C1gCXE4xSPAzY\nAHQiGDBvGMGEqU8RzNvVCJjo7o/Uw1sVkRyj/CVRUkuYRMrMioDzgFPcvRewDhgMHAU86u79gGnA\njWbWmGCk7fPdfSBBkrsrfKkngSvcvT/wJsG8bwCdgVFAd6AL0A24EJjn7gOA/kDjiN+miOQg5S+J\nmoowidoA4BjgDTObBvQF+gFfuvuscJt3CM4EjwNWuvvScPk0gnm4DgOaufvHAO4+3t2fCbcpdvfN\n7l5OMJVEM4LkN9jMHgPOJpgOQ0Skpgag/CURyos7AMl524DJ7n5txQIz6wDMztomQzDPVuVr49nL\nqzph2FF5H3efZ2adCM4iLwCuJ5hIVUSkJpS/JFJqCZOovQOcEU7aipldTdDnocDMuobb9AXmEEwC\nfLiZtQ+XDwZmuPuXQImZ9Qhf48bwdfbIzL4L9HD3KcDVQHsz0wmHiNSU8pdESh+sRMrdZ5rZBGCa\nmW0FlhM00y8DRpjZWIKTgYvcfYuZjQSeNbNtwEZgZPhSlwH3mtl2gn4ZlxF0Yt2TT4EHw9fIAH9w\n98pnnCIie6X8JVHT3ZFS78Lm/Onu3jbuWEREakL5S+qSLkeKiIiIxEAtYSIiIiIxUEuYiIiISAxU\nhImIiIjEQEWYiIiISAxUhImIiIjEQEWYiIiISAz+C2SB3isj6BcGAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09e57be320>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOTTING\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for filename in os.listdir(savedir):\n",
    "    if filename.endswith('.pkl'):\n",
    "        with open(os.path.join(savedir, filename),'rb') as fin:\n",
    "            results = pickle.load(fin)\n",
    "            ax1.plot(results['loss'])\n",
    "            ax1.set_ylabel('cross entropy')\n",
    "            ax1.set_xlabel('epochs')\n",
    "            \n",
    "            ax2.plot(results['accuracy'], label = filename[:-4])\n",
    "            ax2.set_ylabel('accuracy')\n",
    "            ax2.set_xlabel('epochs')\n",
    "            \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHMhlY9re3dl"
   },
   "outputs": [],
   "source": [
    "class AwesomeNet(nn.Module):\n",
    "    \"\"\"The MNIST killer net.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:,:,0,:10]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3673
    },
    "colab_type": "code",
    "id": "nkfMFV1-e3dq",
    "outputId": "7837d367-5de8-4cc4-b9c7-1cd82be81029"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/45056 (0%)]\tLoss: 2.351320\n",
      "Train Epoch: 1 [640/45056 (1%)]\tLoss: 0.414239\n",
      "Train Epoch: 1 [1280/45056 (3%)]\tLoss: 0.124123\n",
      "Train Epoch: 1 [1920/45056 (4%)]\tLoss: 0.143542\n",
      "Train Epoch: 1 [2560/45056 (6%)]\tLoss: 0.131083\n",
      "Train Epoch: 1 [3200/45056 (7%)]\tLoss: 0.054203\n",
      "Train Epoch: 1 [3840/45056 (9%)]\tLoss: 0.190858\n",
      "Train Epoch: 1 [4480/45056 (10%)]\tLoss: 0.120072\n",
      "Train Epoch: 1 [5120/45056 (11%)]\tLoss: 0.066540\n",
      "Train Epoch: 1 [5760/45056 (13%)]\tLoss: 0.115501\n",
      "Train Epoch: 1 [6400/45056 (14%)]\tLoss: 0.109241\n",
      "Train Epoch: 1 [7040/45056 (16%)]\tLoss: 0.022404\n",
      "Train Epoch: 1 [7680/45056 (17%)]\tLoss: 0.116390\n",
      "Train Epoch: 1 [8320/45056 (18%)]\tLoss: 0.043814\n",
      "Train Epoch: 1 [8960/45056 (20%)]\tLoss: 0.052841\n",
      "Train Epoch: 1 [9600/45056 (21%)]\tLoss: 0.010785\n",
      "Train Epoch: 1 [10240/45056 (23%)]\tLoss: 0.103842\n",
      "Train Epoch: 1 [10880/45056 (24%)]\tLoss: 0.085207\n",
      "Train Epoch: 1 [11520/45056 (26%)]\tLoss: 0.167525\n",
      "Train Epoch: 1 [12160/45056 (27%)]\tLoss: 0.031070\n",
      "Train Epoch: 1 [12800/45056 (28%)]\tLoss: 0.027959\n",
      "Train Epoch: 1 [13440/45056 (30%)]\tLoss: 0.147993\n",
      "Train Epoch: 1 [14080/45056 (31%)]\tLoss: 0.075696\n",
      "Train Epoch: 1 [14720/45056 (33%)]\tLoss: 0.042119\n",
      "Train Epoch: 1 [15360/45056 (34%)]\tLoss: 0.014165\n",
      "Train Epoch: 1 [16000/45056 (36%)]\tLoss: 0.134478\n",
      "Train Epoch: 1 [16640/45056 (37%)]\tLoss: 0.036636\n",
      "Train Epoch: 1 [17280/45056 (38%)]\tLoss: 0.130465\n",
      "Train Epoch: 1 [17920/45056 (40%)]\tLoss: 0.116500\n",
      "Train Epoch: 1 [18560/45056 (41%)]\tLoss: 0.016727\n",
      "Train Epoch: 1 [19200/45056 (43%)]\tLoss: 0.123705\n",
      "Train Epoch: 1 [19840/45056 (44%)]\tLoss: 0.053993\n",
      "Train Epoch: 1 [20480/45056 (45%)]\tLoss: 0.113519\n",
      "Train Epoch: 1 [21120/45056 (47%)]\tLoss: 0.037043\n",
      "Train Epoch: 1 [21760/45056 (48%)]\tLoss: 0.032031\n",
      "Train Epoch: 1 [22400/45056 (50%)]\tLoss: 0.013126\n",
      "Train Epoch: 1 [23040/45056 (51%)]\tLoss: 0.099267\n",
      "Train Epoch: 1 [23680/45056 (53%)]\tLoss: 0.119943\n",
      "Train Epoch: 1 [24320/45056 (54%)]\tLoss: 0.118235\n",
      "Train Epoch: 1 [24960/45056 (55%)]\tLoss: 0.013405\n",
      "Train Epoch: 1 [25600/45056 (57%)]\tLoss: 0.064944\n",
      "Train Epoch: 1 [26240/45056 (58%)]\tLoss: 0.053555\n",
      "Train Epoch: 1 [26880/45056 (60%)]\tLoss: 0.030690\n",
      "Train Epoch: 1 [27520/45056 (61%)]\tLoss: 0.235306\n",
      "Train Epoch: 1 [28160/45056 (62%)]\tLoss: 0.086849\n",
      "Train Epoch: 1 [28800/45056 (64%)]\tLoss: 0.008123\n",
      "Train Epoch: 1 [29440/45056 (65%)]\tLoss: 0.033689\n",
      "Train Epoch: 1 [30080/45056 (67%)]\tLoss: 0.011815\n",
      "Train Epoch: 1 [30720/45056 (68%)]\tLoss: 0.084176\n",
      "Train Epoch: 1 [31360/45056 (70%)]\tLoss: 0.037100\n",
      "Train Epoch: 1 [32000/45056 (71%)]\tLoss: 0.083814\n",
      "Train Epoch: 1 [32640/45056 (72%)]\tLoss: 0.077018\n",
      "Train Epoch: 1 [33280/45056 (74%)]\tLoss: 0.232472\n",
      "Train Epoch: 1 [33920/45056 (75%)]\tLoss: 0.063800\n",
      "Train Epoch: 1 [34560/45056 (77%)]\tLoss: 0.031006\n",
      "Train Epoch: 1 [35200/45056 (78%)]\tLoss: 0.029327\n",
      "Train Epoch: 1 [35840/45056 (80%)]\tLoss: 0.011550\n",
      "Train Epoch: 1 [36480/45056 (81%)]\tLoss: 0.027111\n",
      "Train Epoch: 1 [37120/45056 (82%)]\tLoss: 0.088557\n",
      "Train Epoch: 1 [37760/45056 (84%)]\tLoss: 0.025178\n",
      "Train Epoch: 1 [38400/45056 (85%)]\tLoss: 0.067032\n",
      "Train Epoch: 1 [39040/45056 (87%)]\tLoss: 0.015741\n",
      "Train Epoch: 1 [39680/45056 (88%)]\tLoss: 0.011755\n",
      "Train Epoch: 1 [40320/45056 (89%)]\tLoss: 0.027568\n",
      "Train Epoch: 1 [40960/45056 (91%)]\tLoss: 0.061824\n",
      "Train Epoch: 1 [41600/45056 (92%)]\tLoss: 0.034311\n",
      "Train Epoch: 1 [42240/45056 (94%)]\tLoss: 0.119540\n",
      "Train Epoch: 1 [42880/45056 (95%)]\tLoss: 0.150137\n",
      "Train Epoch: 1 [43520/45056 (97%)]\tLoss: 0.140342\n",
      "Train Epoch: 1 [44160/45056 (98%)]\tLoss: 0.049057\n",
      "Train Epoch: 1 [44800/45056 (99%)]\tLoss: 0.116145\n",
      "\n",
      "Test set: Average loss: 0.0535, Accuracy: 14751/15000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/45056 (0%)]\tLoss: 0.005969\n",
      "Train Epoch: 2 [640/45056 (1%)]\tLoss: 0.019875\n",
      "Train Epoch: 2 [1280/45056 (3%)]\tLoss: 0.064120\n",
      "Train Epoch: 2 [1920/45056 (4%)]\tLoss: 0.072775\n",
      "Train Epoch: 2 [2560/45056 (6%)]\tLoss: 0.145474\n",
      "Train Epoch: 2 [3200/45056 (7%)]\tLoss: 0.042443\n",
      "Train Epoch: 2 [3840/45056 (9%)]\tLoss: 0.023272\n",
      "Train Epoch: 2 [4480/45056 (10%)]\tLoss: 0.017176\n",
      "Train Epoch: 2 [5120/45056 (11%)]\tLoss: 0.014431\n",
      "Train Epoch: 2 [5760/45056 (13%)]\tLoss: 0.034669\n",
      "Train Epoch: 2 [6400/45056 (14%)]\tLoss: 0.027536\n",
      "Train Epoch: 2 [7040/45056 (16%)]\tLoss: 0.082844\n",
      "Train Epoch: 2 [7680/45056 (17%)]\tLoss: 0.168941\n",
      "Train Epoch: 2 [8320/45056 (18%)]\tLoss: 0.022390\n",
      "Train Epoch: 2 [8960/45056 (20%)]\tLoss: 0.002231\n",
      "Train Epoch: 2 [9600/45056 (21%)]\tLoss: 0.015559\n",
      "Train Epoch: 2 [10240/45056 (23%)]\tLoss: 0.015114\n",
      "Train Epoch: 2 [10880/45056 (24%)]\tLoss: 0.112583\n",
      "Train Epoch: 2 [11520/45056 (26%)]\tLoss: 0.038850\n",
      "Train Epoch: 2 [12160/45056 (27%)]\tLoss: 0.038235\n",
      "Train Epoch: 2 [12800/45056 (28%)]\tLoss: 0.164762\n",
      "Train Epoch: 2 [13440/45056 (30%)]\tLoss: 0.021239\n",
      "Train Epoch: 2 [14080/45056 (31%)]\tLoss: 0.003006\n",
      "Train Epoch: 2 [14720/45056 (33%)]\tLoss: 0.066395\n",
      "Train Epoch: 2 [15360/45056 (34%)]\tLoss: 0.002880\n",
      "Train Epoch: 2 [16000/45056 (36%)]\tLoss: 0.023281\n",
      "Train Epoch: 2 [16640/45056 (37%)]\tLoss: 0.008870\n",
      "Train Epoch: 2 [17280/45056 (38%)]\tLoss: 0.009816\n",
      "Train Epoch: 2 [17920/45056 (40%)]\tLoss: 0.007644\n",
      "Train Epoch: 2 [18560/45056 (41%)]\tLoss: 0.145161\n",
      "Train Epoch: 2 [19200/45056 (43%)]\tLoss: 0.007716\n",
      "Train Epoch: 2 [19840/45056 (44%)]\tLoss: 0.114530\n",
      "Train Epoch: 2 [20480/45056 (45%)]\tLoss: 0.026497\n",
      "Train Epoch: 2 [21120/45056 (47%)]\tLoss: 0.054651\n",
      "Train Epoch: 2 [21760/45056 (48%)]\tLoss: 0.040489\n",
      "Train Epoch: 2 [22400/45056 (50%)]\tLoss: 0.033672\n",
      "Train Epoch: 2 [23040/45056 (51%)]\tLoss: 0.028811\n",
      "Train Epoch: 2 [23680/45056 (53%)]\tLoss: 0.067265\n",
      "Train Epoch: 2 [24320/45056 (54%)]\tLoss: 0.018466\n",
      "Train Epoch: 2 [24960/45056 (55%)]\tLoss: 0.043438\n",
      "Train Epoch: 2 [25600/45056 (57%)]\tLoss: 0.067329\n",
      "Train Epoch: 2 [26240/45056 (58%)]\tLoss: 0.079106\n",
      "Train Epoch: 2 [26880/45056 (60%)]\tLoss: 0.004868\n",
      "Train Epoch: 2 [27520/45056 (61%)]\tLoss: 0.106202\n",
      "Train Epoch: 2 [28160/45056 (62%)]\tLoss: 0.088337\n",
      "Train Epoch: 2 [28800/45056 (64%)]\tLoss: 0.015551\n",
      "Train Epoch: 2 [29440/45056 (65%)]\tLoss: 0.025787\n",
      "Train Epoch: 2 [30080/45056 (67%)]\tLoss: 0.064499\n",
      "Train Epoch: 2 [30720/45056 (68%)]\tLoss: 0.067479\n",
      "Train Epoch: 2 [31360/45056 (70%)]\tLoss: 0.026336\n",
      "Train Epoch: 2 [32000/45056 (71%)]\tLoss: 0.017198\n",
      "Train Epoch: 2 [32640/45056 (72%)]\tLoss: 0.003020\n",
      "Train Epoch: 2 [33280/45056 (74%)]\tLoss: 0.066117\n",
      "Train Epoch: 2 [33920/45056 (75%)]\tLoss: 0.028987\n",
      "Train Epoch: 2 [34560/45056 (77%)]\tLoss: 0.076850\n",
      "Train Epoch: 2 [35200/45056 (78%)]\tLoss: 0.012619\n",
      "Train Epoch: 2 [35840/45056 (80%)]\tLoss: 0.074887\n",
      "Train Epoch: 2 [36480/45056 (81%)]\tLoss: 0.018718\n",
      "Train Epoch: 2 [37120/45056 (82%)]\tLoss: 0.024905\n",
      "Train Epoch: 2 [37760/45056 (84%)]\tLoss: 0.002777\n",
      "Train Epoch: 2 [38400/45056 (85%)]\tLoss: 0.069109\n",
      "Train Epoch: 2 [39040/45056 (87%)]\tLoss: 0.016637\n",
      "Train Epoch: 2 [39680/45056 (88%)]\tLoss: 0.011287\n",
      "Train Epoch: 2 [40320/45056 (89%)]\tLoss: 0.006343\n",
      "Train Epoch: 2 [40960/45056 (91%)]\tLoss: 0.109630\n",
      "Train Epoch: 2 [41600/45056 (92%)]\tLoss: 0.005936\n",
      "Train Epoch: 2 [42240/45056 (94%)]\tLoss: 0.037134\n",
      "Train Epoch: 2 [42880/45056 (95%)]\tLoss: 0.065646\n",
      "Train Epoch: 2 [43520/45056 (97%)]\tLoss: 0.165626\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d55f4d74f806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-bbb2acfdb9fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "model = HeavyNet().to(device)\n",
    "\n",
    "lr = 0.005\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "results = {'name':'awesome', 'lr': lr, 'loss': [], 'accuracy':[]}\n",
    "savefile = os.path.join(savedir, results['name']+str(results['lr'])+'.pkl' )\n",
    "\n",
    "for epoch in range(1, 10):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, valid_loader)\n",
    "    \n",
    "    # save results\n",
    "    results['loss'].append(loss)\n",
    "    results['accuracy'].append(acc)\n",
    "    with open(savefile, 'wb') as fout:\n",
    "        pickle.dump(results, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-2QeRnme3dw"
   },
   "outputs": [],
   "source": [
    "class HeavyNet(nn.Module):\n",
    "    \"\"\"A medium sized network that performs very well on MNIST.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # conv block 1\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # conv block 2\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(64*7*7, 512)\n",
    "        self.bn5 = nn.BatchNorm1d(512)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x is [batch_size, channels, heigth, width] = [bs, 1, 28, 28]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.max_pool2d(x, 2) # x is [bs, 32, 14, 14]\n",
    "        \n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = F.max_pool2d(x, 2) # x is [bs, 64, 7, 7]\n",
    "        \n",
    "        x = x.view(x.size(0), -1) # flatten\n",
    "        \n",
    "        x = F.relu(self.bn5(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRtZ1SnTe3dz"
   },
   "source": [
    "If you want to learn more about Pytorch, here is a very comprehensive [tutorial](https://nbviewer.jupyter.org/github/ds4dm/tipsntricks/blob/master/pytorch/tutorial.ipynb) made by Mila for Mila. You are encouraged to look at it **after** this lab session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc6WOP4We3d1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "11_convnets_solution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
